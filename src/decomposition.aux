\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\citation{liu2016ssd}
\citation{schroff2015facenet}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{3}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces The fish-eye camera is mounted in a lamp, so the view is directly from above.\relax }}{4}{figure.caption.3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:stream3}{{1.1}{4}{The fish-eye camera is mounted in a lamp, so the view is directly from above.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Problem statement}{4}{subsection.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Overview of methodology}{4}{subsection.4}}
\citation{piccardi2004background}
\citation{optical-flow}
\citation{haar}
\citation{liu2016ssd}
\citation{schroff2015facenet}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Contribution}{5}{subsection.5}}
\citation{lecun-mnisthandwrittendigit-2010}
\citation{deng2009imagenet}
\citation{Everingham10}
\citation{boiman2008defense,zhang2006svm}
\citation{bosch2007image}
\citation{opelt2004weak}
\citation{chapelle1999support}
\citation{lazebnik2006beyond,nowak2006sampling}
\citation{yang2009linear,bicego2006use}
\citation{munder2006experimental}
\citation{lecun1989backpropagation}
\citation{krizhevsky2012imagenet}
\citation{zeiler2014visualizing}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related work}{6}{section.6}}
\newlabel{sec:related_work}{{2}{6}{Related work}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Classification}{6}{subsection.7}}
\citation{lecun1998gradient}
\citation{schroff2015facenet}
\citation{simonyan2014very}
\citation{liu2016ssd}
\citation{he2016deep}
\citation{zagoruyko2016wide}
\citation{xie2017aggregated}
\citation{huang2017densely}
\citation{daubaras2012vehicle,caruso1999vehicle}
\citation{gate2009fast}
\citation{wender20083d,premebida2007lidar}
\citation{kim2005front,wang2003online}
\citation{bertozzi2000stereo,toulminet2006vehicle}
\citation{apollo-scape,madhavan2017bdd,RobotCarDatasetIJRR,ncarlevaris-2015a}
\citation{tzomakas1998vehicle}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Detection}{7}{subsection.8}}
\newlabel{sec:detection}{{2.2}{7}{Detection}{subsection.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Vehicle detection}{7}{subsubsection.9}}
\citation{haar,lienhart2002extended,viola2004robust}
\citation{sun2002real}
\citation{piccardi2004background,horprasert1999statistical}
\citation{naoya1990optical,quenot1992orthogonal,chen2011tracking}
\citation{lowe2004distinctive}
\citation{girshick2014rich,wang2009hog,zhu2006fast,felzenszwalb2010object,dalal2005histograms}
\citation{girshick2016region}
\citation{DBLP:journals/corr/GirshickDDM13}
\citation{carreira2012cpmc}
\citation{simonyan2014very}
\citation{girshick2015fast}
\citation{redmon2016you}
\citation{redmon2017yolo9000,redmon2018yolov3}
\citation{liu2016ssd}
\citation{erhan2014scalable}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Object detection}{8}{subsubsection.10}}
\newlabel{sec:object_detection}{{2.2.2}{8}{Object detection}{subsubsection.10}{}}
\citation{lin2017focal,li2017fssd,dai2016r}
\citation{sudowe2011efficient}
\citation{gonzalez2012digital}
\citation{naoya1990optical}
\citation{karpathy2014large}
\citation{bayona2010stationary,koprinska2001temporal}
\citation{yilmaz2006object}
\citation{kale2015moving}
\citation{comaniciu2003kernel,porikli2005multi,yilmaz2007object,elgammal2002background}
\citation{isard2001bramble}
\citation{kale2015moving}
\citation{veenman1998fast}
\citation{banerjee2008multi}
\citation{zhong2012moving}
\citation{kale2015moving,mae1996object}
\citation{bertinetto2016fully,held2016learning,gladh2016deep,gaidon2016virtual,lee2016globally}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Results on PASCAL VOC2007 test.\relax }}{9}{table.caption.11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Object tracking}{9}{subsection.12}}
\citation{tan2005introduction,benfold2011stable}
\citation{chen2011tracking}
\citation{joshi2012survey,elgammal2002background}
\citation{makris2004bridging}
\citation{khan2003consistent,krumm2000multi,zhao2005real}
\citation{javed2005appearance,porikli2003inter}
\citation{rahimi2004simultaneous}
\citation{makris2004bridging}
\citation{javed2005appearance,huang1997object}
\citation{kettnaker1999bayesian}
\citation{kang2003continuous}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Reidentification}{10}{subsection.13}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Fish-eye camera model}{10}{section.14}}
\newlabel{sec:lens}{{3}{10}{Fish-eye camera model}{section.14}{}}
\citation{lukacs1997real}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Frame of the provided video\relax }}{11}{figure.caption.15}}
\newlabel{fig:stream1}{{3.1}{11}{Frame of the provided video\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Scene localization}{11}{subsection.16}}
\newlabel{sec:scene_localization}{{3.1}{11}{Scene localization}{subsection.16}{}}
\newlabel{eq:ellipse}{{1}{11}{Scene localization}{equation.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Camera model}{12}{subsection.19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Linear model}{14}{subsubsection.23}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Tangent model}{14}{subsubsection.26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}The city coordinate system}{14}{subsection.29}}
\citation{piccardi2004background}
\citation{horprasert1999statistical,zivkovic2006efficient}
\citation{opencv}
\citation{wren1997pfinder}
\@writefile{toc}{\contentsline {section}{\numberline {4}Dataset generation}{15}{section.30}}
\newlabel{sec:classical}{{4}{15}{Dataset generation}{section.30}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Background subtraction detection}{15}{subsection.31}}
\newlabel{sec:bgs}{{4.1}{15}{Background subtraction detection}{subsection.31}{}}
\citation{bgs-med1,bgs-med2}
\citation{opencv}
\citation{optical-flow}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Optical Flow tracking}{17}{subsection.37}}
\newlabel{sec:optical-flow}{{4.2}{17}{Optical Flow tracking}{subsection.37}{}}
\citation{lucas-kanede}
\citation{shi-tomasi}
\citation{harris}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}classification}{19}{subsection.38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Semi-supervised data generation}{19}{subsection.39}}
\newlabel{sec:data-generation}{{4.4}{19}{Semi-supervised data generation}{subsection.39}{}}
\citation{szegedy2016rethinking}
\citation{graves2013speech}
\citation{rosenblatt1958perceptron}
\@writefile{toc}{\contentsline {section}{\numberline {5}Deep learning}{20}{section.41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Inspiration by biology}{20}{subsection.42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Layers}{20}{subsection.43}}
\citation{karpathy2016cs231n}
\citation{karpathy2016cs231n}
\citation{zeiler2014visualizing}
\citation{karpathy2016cs231n}
\citation{karpathy2016cs231n}
\citation{scherer2010evaluation}
\citation{blumer1989learnability}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Convolutional layer}{21}{subsection.44}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Pooling layer}{21}{subsubsection.46}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Fully connected layer}{21}{subsubsection.48}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3}Overfitting and dropout layer}{21}{subsubsection.49}}
\citation{widrow199030}
\citation{abadi2016tensorflow}
\citation{bergstra2011theano}
\citation{seide2016cntk}
\citation{chollet2015keras}
\citation{jia2014caffe}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Backpropagation}{22}{subsection.50}}
\newlabel{sec:back-prop}{{5.4}{22}{Backpropagation}{subsection.50}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Frameworks}{22}{subsection.53}}
\citation{simonyan2014very}
\citation{simonyan2014very}
\citation{simonyan2014very}
\citation{redmon2016you}
\citation{barz2017fast}
\citation{liu2016ssd}
\citation{faster-rcnn}
\citation{ciresan2011flexible}
\citation{krizhevsky2012imagenet}
\citation{simonyan2014very}
\citation{krizhevsky2012imagenet}
\citation{zeiler2014visualizing}
\citation{lecun1989backpropagation}
\citation{simonyan2014very}
\citation{liu2016ssd}
\@writefile{toc}{\contentsline {section}{\numberline {6}Classification, Detection and Reidentification networks}{23}{section.54}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Neural networks for classification}{23}{subsection.55}}
\newlabel{sec:neural_networks_classification}{{6.1}{23}{Neural networks for classification}{subsection.55}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}*** conv layers, ..., ?}{23}{subsubsection.56}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.2}VGG}{23}{subsubsection.57}}
\newlabel{sec:vgg}{{6.1.2}{23}{VGG}{subsubsection.57}{}}
\citation{liu2016ssd}
\citation{redmon2016you}
\citation{liu2016ssd}
\citation{redmon2016you}
\citation{erhan2014scalable}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}SSD network for detection}{24}{subsection.59}}
\newlabel{sec:ssd}{{6.2}{24}{SSD network for detection}{subsection.59}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.1}Architecture}{24}{subsubsection.60}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.2.2}Default boxes and aspect ratios}{24}{subsubsection.62}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Loss}{24}{subsection.64}}
\citation{diez2011non}
\citation{diez2011non}
\citation{tan2005introduction}
\citation{schroff2015facenet}
\citation{schroff2015facenet}
\citation{schroff2015facenet}
\citation{zeiler2014visualizing}
\citation{szegedy2016rethinking}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}Training}{25}{subsubsection.65}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Non maxima suppression}{25}{subsection.66}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Facenet for reidentification}{25}{subsection.68}}
\newlabel{sec:facenet}{{6.5}{25}{Facenet for reidentification}{subsection.68}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.1}Architecture}{25}{subsubsection.69}}
\citation{szegedy2016rethinking}
\citation{wst2008deeply,taigman2014deepface}
\citation{szegedy2016rethinking}
\citation{chen2017beyond}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.2}Training}{26}{subsubsection.71}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.6}Multi camera tracking}{27}{subsection.74}}
\newlabel{sec:multi-camera-tracking}{{6.6}{27}{Multi camera tracking}{subsection.74}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.1}City representation}{27}{subsubsection.75}}
\citation{he2017mask}
\citation{he2017mask}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.2}Reidentification}{28}{subsubsection.77}}
\newlabel{sec:reidentification}{{6.6.2}{28}{Reidentification}{subsubsection.77}{}}
\newlabel{eq:score-reid}{{11}{28}{Reidentification}{equation.78}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.6.3}Decreasing computational demands}{28}{subsubsection.79}}
\citation{he2017mask}
\citation{liu2016ssd}
\citation{faster-rcnn}
\citation{redmon2016you}
\citation{ssd_keras}
\@writefile{toc}{\contentsline {section}{\numberline {7}Implementation}{29}{section.81}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Detection and tracking on CPU}{29}{subsection.82}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Mask R-CNN segmentation}{29}{subsection.83}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}SSD detector}{29}{subsection.85}}
\newlabel{sec:ssd-implementation}{{7.3}{29}{SSD detector}{subsection.85}{}}
\citation{walt2011numpy}
\citation{opencv}
\citation{cao2018feature}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}Temporal difference}{30}{subsubsection.86}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}Architecture}{30}{subsubsection.89}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.3}Dataset}{31}{subsubsection.90}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.4}Data augmentation}{31}{subsubsection.91}}
\citation{deng2009imagenet}
\citation{lin2014microsoft}
\citation{real2017youtube}
\citation{kingma2014adam}
\citation{ssd_keras}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.5}Training}{32}{subsubsection.92}}
\newlabel{fig:ssd_mAP}{{7.3a}{33}{The mean average precision on validation set.\relax }{figure.caption.93}{}}
\newlabel{sub@fig:ssd_mAP}{{a}{33}{The mean average precision on validation set.\relax }{figure.caption.93}{}}
\newlabel{fig:ssd_loss}{{7.3b}{33}{The loss on the training set.\relax }{figure.caption.93}{}}
\newlabel{sub@fig:ssd_loss}{{b}{33}{The loss on the training set.\relax }{figure.caption.93}{}}
\newlabel{fig:ssd_val_loss}{{7.3c}{33}{The loss on the validation set.\relax }{figure.caption.93}{}}
\newlabel{sub@fig:ssd_val_loss}{{c}{33}{The loss on the validation set.\relax }{figure.caption.93}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces The process of training the 4 channel input SSD network. The training was performed on the NVIDIA GeForce GTX 1080 for 4 days.\relax }}{33}{figure.caption.93}}
\newlabel{fig:ssd_training}{{7.3}{33}{The process of training the 4 channel input SSD network. The training was performed on the NVIDIA GeForce GTX 1080 for 4 days.\relax }{figure.caption.93}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Tracking}{33}{subsection.94}}
\citation{shi-tomasi}
\citation{szegedy2016rethinking}
\citation{huang2007labeled}
\citation{wolf2011face}
\citation{szegedy2016rethinking}
\citation{huang2007labeled}
\citation{wolf2011face}
\citation{schroff2015facenet}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.1}Seeding}{34}{subsubsection.95}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.2}Displacement}{34}{subsubsection.96}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.4.3}Matching}{34}{subsubsection.97}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Similarity}{34}{subsection.98}}
\newlabel{sec:similarity}{{7.5}{34}{Similarity}{subsection.98}{}}
\citation{facenet}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.1}Dataset}{35}{subsubsection.100}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.2}Problems with the dataset}{35}{subsubsection.102}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.3}Improving the dataset}{36}{subsubsection.103}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.4}Training}{36}{subsubsection.104}}
\citation{wold1987principal}
\citation{maaten2008visualizing}
\citation{pedregosa2011scikit}
\newlabel{fig:facenet}{{7.7a}{37}{True positive rate.\relax }{figure.caption.106}{}}
\newlabel{sub@fig:facenet}{{a}{37}{True positive rate.\relax }{figure.caption.106}{}}
\newlabel{fig:facenet}{{7.7b}{37}{True negative rate.\relax }{figure.caption.106}{}}
\newlabel{sub@fig:facenet}{{b}{37}{True negative rate.\relax }{figure.caption.106}{}}
\newlabel{fig:facenet}{{7.7c}{37}{False positive rate.\relax }{figure.caption.106}{}}
\newlabel{sub@fig:facenet}{{c}{37}{False positive rate.\relax }{figure.caption.106}{}}
\newlabel{fig:facenet}{{7.7d}{37}{False negative rate.\relax }{figure.caption.106}{}}
\newlabel{sub@fig:facenet}{{d}{37}{False negative rate.\relax }{figure.caption.106}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces Additional information about the Facenet training. The training was performed on the NVIDIA GeForce GTX 1080 for 12 hours.\relax }}{37}{figure.caption.106}}
\newlabel{fig:facenet_training_additional}{{7.7}{37}{Additional information about the Facenet training. The training was performed on the NVIDIA GeForce GTX 1080 for 12 hours.\relax }{figure.caption.106}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.5.5}T-SNE visualization}{37}{subsubsection.107}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.6}Multi camera tracking}{38}{subsection.109}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Evaluation}{38}{section.110}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.0.1}Mean average precision.}{38}{subsubsection.111}}
\newlabel{sec:mAP}{{8.0.1}{38}{Mean average precision}{subsubsection.111}{}}
\citation{liu2016ssd}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Example of precision-recall curve\relax }}{39}{figure.caption.115}}
\newlabel{fig:precision-recall}{{8.1}{39}{Example of precision-recall curve\relax }{figure.caption.115}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}SSD object detection}{40}{subsection.116}}
\newlabel{fig:ssd_mAP}{{8.2a}{41}{The mean average precision on validation set.\relax }{figure.caption.117}{}}
\newlabel{sub@fig:ssd_mAP}{{a}{41}{The mean average precision on validation set.\relax }{figure.caption.117}{}}
\newlabel{fig:ssd_loss}{{8.2b}{41}{The loss on the training set.\relax }{figure.caption.117}{}}
\newlabel{sub@fig:ssd_loss}{{b}{41}{The loss on the training set.\relax }{figure.caption.117}{}}
\newlabel{fig:ssd_val_loss}{{8.2c}{41}{The loss on the validation set.\relax }{figure.caption.117}{}}
\newlabel{sub@fig:ssd_val_loss}{{c}{41}{The loss on the validation set.\relax }{figure.caption.117}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces The process of training the introduced 4 channel SSD (orange) and the 3 channel SSD (blue) networks. The training was performed on the NVIDIA GeForce GTX 1080 for 1 day. The graph shows, that the presented solution performs much better than the state of the art SSD.\relax }}{41}{figure.caption.117}}
\newlabel{fig:ssd_training}{{8.2}{41}{The process of training the introduced 4 channel SSD (orange) and the 3 channel SSD (blue) networks. The training was performed on the NVIDIA GeForce GTX 1080 for 1 day. The graph shows, that the presented solution performs much better than the state of the art SSD.\relax }{figure.caption.117}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Results on the custom dataset from the section \ref  {sec:ssd-dataset} show, that the introduced RGBD architecture is more accurate, than the standard SSD.\relax }}{42}{table.caption.118}}
\newlabel{tab:ssd_camparison}{{2}{42}{Results on the custom dataset from the section \ref {sec:ssd-dataset} show, that the introduced RGBD architecture is more accurate, than the standard SSD.\relax }{table.caption.118}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Facenet similarity}{42}{subsection.119}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.1}Evaluation metrics}{42}{subsubsection.120}}
\newlabel{sec:similarity-eval}{{8.2.1}{42}{Evaluation metrics}{subsubsection.120}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.2.2}Results}{42}{subsubsection.121}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Final results of the facenet training after 12 hours on NVIDIA GeForce GTX 1080.\relax }}{42}{table.caption.122}}
\newlabel{tab:facenet}{{3}{42}{Final results of the facenet training after 12 hours on NVIDIA GeForce GTX 1080.\relax }{table.caption.122}{}}
\bibstyle{plain}
\bibdata{ref}
\bibcite{abadi2016tensorflow}{{1}{}{{}}{{}}}
\bibcite{banerjee2008multi}{{2}{}{{}}{{}}}
\bibcite{barz2017fast}{{3}{}{{}}{{}}}
\bibcite{bayona2010stationary}{{4}{}{{}}{{}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Multi camera tracking}{43}{subsection.123}}
\newlabel{sec:multi-camera-tracking}{{8.3}{43}{Multi camera tracking}{subsection.123}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.3.1}Evaluation measurement}{43}{subsubsection.124}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Conclusion}{43}{section.125}}
\bibcite{optical-flow}{{5}{}{{}}{{}}}
\bibcite{benfold2011stable}{{6}{}{{}}{{}}}
\bibcite{bergstra2011theano}{{7}{}{{}}{{}}}
\bibcite{bertinetto2016fully}{{8}{}{{}}{{}}}
\bibcite{bertozzi2000stereo}{{9}{}{{}}{{}}}
\bibcite{bicego2006use}{{10}{}{{}}{{}}}
\bibcite{blumer1989learnability}{{11}{}{{}}{{}}}
\bibcite{boiman2008defense}{{12}{}{{}}{{}}}
\bibcite{bosch2007image}{{13}{}{{}}{{}}}
\bibcite{opencv}{{14}{}{{}}{{}}}
\bibcite{cao2018feature}{{15}{}{{}}{{}}}
\bibcite{ncarlevaris-2015a}{{16}{}{{}}{{}}}
\bibcite{carreira2012cpmc}{{17}{}{{}}{{}}}
\bibcite{caruso1999vehicle}{{18}{}{{}}{{}}}
\bibcite{chapelle1999support}{{19}{}{{}}{{}}}
\bibcite{chen2017beyond}{{20}{}{{}}{{}}}
\bibcite{chen2011tracking}{{21}{}{{}}{{}}}
\bibcite{chollet2015keras}{{22}{}{{}}{{}}}
\bibcite{ciresan2011flexible}{{23}{}{{}}{{}}}
\bibcite{comaniciu2003kernel}{{24}{}{{}}{{}}}
\bibcite{bgs-med2}{{25}{}{{}}{{}}}
\bibcite{dai2016r}{{26}{}{{}}{{}}}
\bibcite{dalal2005histograms}{{27}{}{{}}{{}}}
\bibcite{daubaras2012vehicle}{{28}{}{{}}{{}}}
\bibcite{deng2009imagenet}{{29}{}{{}}{{}}}
\bibcite{diez2011non}{{30}{}{{}}{{}}}
\bibcite{elgammal2002background}{{31}{}{{}}{{}}}
\bibcite{erhan2014scalable}{{32}{}{{}}{{}}}
\bibcite{Everingham10}{{33}{}{{}}{{}}}
\bibcite{felzenszwalb2010object}{{34}{}{{}}{{}}}
\bibcite{facenet}{{35}{}{{}}{{}}}
\bibcite{ssd_keras}{{36}{}{{}}{{}}}
\bibcite{gaidon2016virtual}{{37}{}{{}}{{}}}
\bibcite{gate2009fast}{{38}{}{{}}{{}}}
\bibcite{girshick2015fast}{{39}{}{{}}{{}}}
\bibcite{girshick2014rich}{{40}{}{{}}{{}}}
\bibcite{girshick2016region}{{41}{}{{}}{{}}}
\bibcite{DBLP:journals/corr/GirshickDDM13}{{42}{}{{}}{{}}}
\bibcite{gladh2016deep}{{43}{}{{}}{{}}}
\bibcite{gonzalez2012digital}{{44}{}{{}}{{}}}
\bibcite{graves2013speech}{{45}{}{{}}{{}}}
\bibcite{harris}{{46}{}{{}}{{}}}
\bibcite{haar}{{47}{}{{}}{{}}}
\bibcite{he2017mask}{{48}{}{{}}{{}}}
\bibcite{he2016deep}{{49}{}{{}}{{}}}
\bibcite{held2016learning}{{50}{}{{}}{{}}}
\bibcite{horprasert1999statistical}{{51}{}{{}}{{}}}
\bibcite{huang2017densely}{{52}{}{{}}{{}}}
\bibcite{huang2007labeled}{{53}{}{{}}{{}}}
\bibcite{huang1997object}{{54}{}{{}}{{}}}
\bibcite{apollo-scape}{{55}{}{{}}{{}}}
\bibcite{isard2001bramble}{{56}{}{{}}{{}}}
\bibcite{javed2005appearance}{{57}{}{{}}{{}}}
\bibcite{jia2014caffe}{{58}{}{{}}{{}}}
\bibcite{joshi2012survey}{{59}{}{{}}{{}}}
\bibcite{kale2015moving}{{60}{}{{}}{{}}}
\bibcite{kang2003continuous}{{61}{}{{}}{{}}}
\bibcite{karpathy2016cs231n}{{62}{}{{}}{{}}}
\bibcite{karpathy2014large}{{63}{}{{}}{{}}}
\bibcite{kettnaker1999bayesian}{{64}{}{{}}{{}}}
\bibcite{khan2003consistent}{{65}{}{{}}{{}}}
\bibcite{kim2005front}{{66}{}{{}}{{}}}
\bibcite{kingma2014adam}{{67}{}{{}}{{}}}
\bibcite{koprinska2001temporal}{{68}{}{{}}{{}}}
\bibcite{krizhevsky2012imagenet}{{69}{}{{}}{{}}}
\bibcite{krumm2000multi}{{70}{}{{}}{{}}}
\bibcite{lazebnik2006beyond}{{71}{}{{}}{{}}}
\bibcite{lecun1989backpropagation}{{72}{}{{}}{{}}}
\bibcite{lecun1998gradient}{{73}{}{{}}{{}}}
\bibcite{lecun-mnisthandwrittendigit-2010}{{74}{}{{}}{{}}}
\bibcite{lee2016globally}{{75}{}{{}}{{}}}
\bibcite{li2017fssd}{{76}{}{{}}{{}}}
\bibcite{lienhart2002extended}{{77}{}{{}}{{}}}
\bibcite{lin2017focal}{{78}{}{{}}{{}}}
\bibcite{lin2014microsoft}{{79}{}{{}}{{}}}
\bibcite{liu2016ssd}{{80}{}{{}}{{}}}
\bibcite{bgs-med1}{{81}{}{{}}{{}}}
\bibcite{lowe2004distinctive}{{82}{}{{}}{{}}}
\bibcite{lucas-kanede}{{83}{}{{}}{{}}}
\bibcite{maaten2008visualizing}{{84}{}{{}}{{}}}
\bibcite{RobotCarDatasetIJRR}{{85}{}{{}}{{}}}
\bibcite{madhavan2017bdd}{{86}{}{{}}{{}}}
\bibcite{mae1996object}{{87}{}{{}}{{}}}
\bibcite{makris2004bridging}{{88}{}{{}}{{}}}
\bibcite{munder2006experimental}{{89}{}{{}}{{}}}
\bibcite{naoya1990optical}{{90}{}{{}}{{}}}
\bibcite{nowak2006sampling}{{91}{}{{}}{{}}}
\bibcite{opelt2004weak}{{92}{}{{}}{{}}}
\bibcite{pedregosa2011scikit}{{93}{}{{}}{{}}}
\bibcite{piccardi2004background}{{94}{}{{}}{{}}}
\bibcite{porikli2003inter}{{95}{}{{}}{{}}}
\bibcite{porikli2005multi}{{96}{}{{}}{{}}}
\bibcite{premebida2007lidar}{{97}{}{{}}{{}}}
\bibcite{quenot1992orthogonal}{{98}{}{{}}{{}}}
\bibcite{rahimi2004simultaneous}{{99}{}{{}}{{}}}
\bibcite{real2017youtube}{{100}{}{{}}{{}}}
\bibcite{redmon2016you}{{101}{}{{}}{{}}}
\bibcite{redmon2017yolo9000}{{102}{}{{}}{{}}}
\bibcite{redmon2018yolov3}{{103}{}{{}}{{}}}
\bibcite{faster-rcnn}{{104}{}{{}}{{}}}
\bibcite{rosenblatt1958perceptron}{{105}{}{{}}{{}}}
\bibcite{scherer2010evaluation}{{106}{}{{}}{{}}}
\bibcite{schroff2015facenet}{{107}{}{{}}{{}}}
\bibcite{seide2016cntk}{{108}{}{{}}{{}}}
\bibcite{shi-tomasi}{{109}{}{{}}{{}}}
\bibcite{simonyan2014very}{{110}{}{{}}{{}}}
\bibcite{sudowe2011efficient}{{111}{}{{}}{{}}}
\bibcite{sun2002real}{{112}{}{{}}{{}}}
\bibcite{szegedy2016rethinking}{{113}{}{{}}{{}}}
\bibcite{taigman2014deepface}{{114}{}{{}}{{}}}
\bibcite{tan2005introduction}{{115}{}{{}}{{}}}
\bibcite{toulminet2006vehicle}{{116}{}{{}}{{}}}
\bibcite{tzomakas1998vehicle}{{117}{}{{}}{{}}}
\bibcite{veenman1998fast}{{118}{}{{}}{{}}}
\bibcite{viola2004robust}{{119}{}{{}}{{}}}
\bibcite{walt2011numpy}{{120}{}{{}}{{}}}
\bibcite{wang2003online}{{121}{}{{}}{{}}}
\bibcite{wang2009hog}{{122}{}{{}}{{}}}
\bibcite{wender20083d}{{123}{}{{}}{{}}}
\bibcite{widrow199030}{{124}{}{{}}{{}}}
\bibcite{wold1987principal}{{125}{}{{}}{{}}}
\bibcite{wolf2011face}{{126}{}{{}}{{}}}
\bibcite{wren1997pfinder}{{127}{}{{}}{{}}}
\bibcite{wst2008deeply}{{128}{}{{}}{{}}}
\bibcite{xie2017aggregated}{{129}{}{{}}{{}}}
\bibcite{yang2009linear}{{130}{}{{}}{{}}}
\bibcite{yilmaz2007object}{{131}{}{{}}{{}}}
\bibcite{yilmaz2006object}{{132}{}{{}}{{}}}
\bibcite{zagoruyko2016wide}{{133}{}{{}}{{}}}
\bibcite{zeiler2014visualizing}{{134}{}{{}}{{}}}
\bibcite{zhang2006svm}{{135}{}{{}}{{}}}
\bibcite{zhao2005real}{{136}{}{{}}{{}}}
\bibcite{zhong2012moving}{{137}{}{{}}{{}}}
\bibcite{zhu2006fast}{{138}{}{{}}{{}}}
\bibcite{zivkovic2006efficient}{{139}{}{{}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces The spherical coordinates of the world\relax }}{56}{figure.caption.20}}
\newlabel{fig:sphere}{{3.2}{56}{The spherical coordinates of the world\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The provided calibration data\relax }}{57}{figure.caption.22}}
\newlabel{fig:calibration}{{3.3}{57}{The provided calibration data\relax }{figure.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The linear model of the lens\relax }}{58}{figure.caption.25}}
\newlabel{fig:linear_model}{{3.4}{58}{The linear model of the lens\relax }{figure.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces The linear model of the lens\relax }}{59}{figure.caption.28}}
\newlabel{fig:linear_model}{{3.5}{59}{The linear model of the lens\relax }{figure.caption.28}{}}
\newlabel{fig:cut_mean}{{4.1a}{59}{Mean model\relax }{figure.caption.32}{}}
\newlabel{sub@fig:cut_mean}{{a}{59}{Mean model\relax }{figure.caption.32}{}}
\newlabel{fig:cut_med}{{4.1b}{59}{Median model\relax }{figure.caption.32}{}}
\newlabel{sub@fig:cut_med}{{b}{59}{Median model\relax }{figure.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Background model created by the mean and the median approach.\relax }}{59}{figure.caption.32}}
\newlabel{fig:threshold_mean}{{4.2a}{60}{Mean\relax }{figure.caption.34}{}}
\newlabel{sub@fig:threshold_mean}{{a}{60}{Mean\relax }{figure.caption.34}{}}
\newlabel{fig:threshold_med}{{4.2b}{60}{Median\relax }{figure.caption.34}{}}
\newlabel{sub@fig:threshold_med}{{b}{60}{Median\relax }{figure.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces The difference between the frame and a background shown in a gray-scale.\relax }}{60}{figure.caption.34}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The histogram of a particular pixel over 100 images with computed medians\relax }}{60}{figure.caption.35}}
\newlabel{fig:pixel_hist}{{4.3}{60}{The histogram of a particular pixel over 100 images with computed medians\relax }{figure.caption.35}{}}
\newlabel{fig:frame_for_detection}{{4.4a}{61}{A frame for detection\relax }{figure.caption.36}{}}
\newlabel{sub@fig:frame_for_detection}{{a}{61}{A frame for detection\relax }{figure.caption.36}{}}
\newlabel{fig:mask_area}{{4.4b}{61}{The detections and areas of contours.\relax }{figure.caption.36}{}}
\newlabel{sub@fig:mask_area}{{b}{61}{The detections and areas of contours.\relax }{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces The background subtraction detection algorithm.\relax }}{61}{figure.caption.36}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces The dialog from the annotation tool.\relax }}{61}{figure.caption.40}}
\newlabel{fig:labeling}{{4.5}{61}{The dialog from the annotation tool.\relax }{figure.caption.40}{}}
\newlabel{fig:conv_layer}{{5.1a}{62}{An example of a convolutional layer\relax }{figure.caption.45}{}}
\newlabel{sub@fig:conv_layer}{{a}{62}{An example of a convolutional layer\relax }{figure.caption.45}{}}
\newlabel{fig:neuron}{{5.1b}{62}{Model of a neuron.\relax }{figure.caption.45}{}}
\newlabel{sub@fig:neuron}{{b}{62}{Model of a neuron.\relax }{figure.caption.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Examples of neural networks concepts from \cite  {karpathy2016cs231n}.\relax }}{62}{figure.caption.45}}
\newlabel{fig:conv_layer}{{5.2a}{62}{Pooling downsamples the previous layer.\relax }{figure.caption.47}{}}
\newlabel{sub@fig:conv_layer}{{a}{62}{Pooling downsamples the previous layer.\relax }{figure.caption.47}{}}
\newlabel{fig:neuron}{{5.2b}{62}{Max pooling operation.\relax }{figure.caption.47}{}}
\newlabel{sub@fig:neuron}{{b}{62}{Max pooling operation.\relax }{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Examples of pooling concepts from \cite  {karpathy2016cs231n}.\relax }}{62}{figure.caption.47}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces The different VGG architectures. The ReLU function is not shown for simplicity. \cite  {simonyan2014very}\relax }}{63}{figure.caption.58}}
\newlabel{fig:vgg}{{6.1}{63}{The different VGG architectures. The ReLU function is not shown for simplicity. \cite {simonyan2014very}\relax }{figure.caption.58}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Comparison of the SSD\cite  {liu2016ssd}(300x300) and YOLO\cite  {redmon2016you}(448x448) architectures.\relax }}{64}{figure.caption.61}}
\newlabel{fig:ssd_vs_yolo}{{6.2}{64}{Comparison of the SSD\cite {liu2016ssd}(300x300) and YOLO\cite {redmon2016you}(448x448) architectures.\relax }{figure.caption.61}{}}
\newlabel{fig:ssd_detection}{{6.3a}{64}{SSD prediction\relax }{figure.caption.63}{}}
\newlabel{sub@fig:ssd_detection}{{a}{64}{SSD prediction\relax }{figure.caption.63}{}}
\newlabel{fig:ssd4}{{6.3b}{64}{8 $\times $ 8 feature map.\relax }{figure.caption.63}{}}
\newlabel{sub@fig:ssd4}{{b}{64}{8 $\times $ 8 feature map.\relax }{figure.caption.63}{}}
\newlabel{fig:ssd8}{{6.3c}{64}{The predicted offsets.\relax }{figure.caption.63}{}}
\newlabel{sub@fig:ssd8}{{c}{64}{The predicted offsets.\relax }{figure.caption.63}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces The background subtraction detection algorithm.\relax }}{64}{figure.caption.63}}
\newlabel{fig:nms1}{{6.4a}{65}{Predictions before applying nms\relax }{figure.caption.67}{}}
\newlabel{sub@fig:nms1}{{a}{65}{Predictions before applying nms\relax }{figure.caption.67}{}}
\newlabel{fig:nms2}{{6.4b}{65}{Predictions after applying nms\relax }{figure.caption.67}{}}
\newlabel{sub@fig:nms2}{{b}{65}{Predictions after applying nms\relax }{figure.caption.67}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Non maxima suppression\cite  {diez2011non} keeps a single prediction.\relax }}{65}{figure.caption.67}}
\newlabel{fig:nms}{{6.4}{65}{Non maxima suppression\cite {diez2011non} keeps a single prediction.\relax }{figure.caption.67}{}}
\newlabel{fig:facenet_structure}{{6.5a}{65}{The model structure\relax }{figure.caption.70}{}}
\newlabel{sub@fig:facenet_structure}{{a}{65}{The model structure\relax }{figure.caption.70}{}}
\newlabel{fig:triplet_loss}{{6.5b}{65}{The learning objective\relax }{figure.caption.70}{}}
\newlabel{sub@fig:triplet_loss}{{b}{65}{The learning objective\relax }{figure.caption.70}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces The Facenet \cite  {schroff2015facenet}.\relax }}{65}{figure.caption.70}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces An example of a part of city representation with transition probabilities.\relax }}{65}{figure.caption.76}}
\newlabel{fig:intersection}{{6.6}{65}{An example of a part of city representation with transition probabilities.\relax }{figure.caption.76}{}}
\newlabel{fig:mask-street-cam}{{7.1a}{66}{Mask R-CNN on a standard street camera\relax }{figure.caption.84}{}}
\newlabel{sub@fig:mask-street-cam}{{a}{66}{Mask R-CNN on a standard street camera\relax }{figure.caption.84}{}}
\newlabel{fig:mask-omni-cam}{{7.1b}{66}{Mask R-CNN on a fish-eye has problems recognizing vehicles from the top\relax }{figure.caption.84}{}}
\newlabel{sub@fig:mask-omni-cam}{{b}{66}{Mask R-CNN on a fish-eye has problems recognizing vehicles from the top\relax }{figure.caption.84}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Examples of Mask R-CNN network \cite  {he2017mask}\relax }}{66}{figure.caption.84}}
\newlabel{fig:facenet}{{7.1}{66}{Examples of Mask R-CNN network \cite {he2017mask}\relax }{figure.caption.84}{}}
\newlabel{fig:td_frame}{{7.2a}{66}{A frame from the street camera.\relax }{figure.caption.87}{}}
\newlabel{sub@fig:td_frame}{{a}{66}{A frame from the street camera.\relax }{figure.caption.87}{}}
\newlabel{fig:td_black}{{7.2b}{66}{Temporal difference highlights moving objects.\relax }{figure.caption.87}{}}
\newlabel{sub@fig:td_black}{{b}{66}{Temporal difference highlights moving objects.\relax }{figure.caption.87}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Temporal difference helps to detect moving objects.\relax }}{66}{figure.caption.87}}
\newlabel{fig:td}{{7.2}{66}{Temporal difference helps to detect moving objects.\relax }{figure.caption.87}{}}
\newlabel{fig:facenet_table}{{7.4a}{67}{Mean VAL at $10^{-3}$ FAR.\relax }{figure.caption.99}{}}
\newlabel{sub@fig:facenet_table}{{a}{67}{Mean VAL at $10^{-3}$ FAR.\relax }{figure.caption.99}{}}
\newlabel{fig:triplet_loss}{{7.4b}{67}{Comparison of different DNN architectures.\relax }{figure.caption.99}{}}
\newlabel{sub@fig:triplet_loss}{{b}{67}{Comparison of different DNN architectures.\relax }{figure.caption.99}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Comparison of various facenet\cite  {szegedy2016rethinking} base networks evaluated on Labeled faces in the wild\cite  {huang2007labeled} and YouTube faces\cite  {wolf2011face} datasets.\relax }}{67}{figure.caption.99}}
\newlabel{fig:facenet}{{7.4}{67}{Comparison of various facenet\cite {szegedy2016rethinking} base networks evaluated on Labeled faces in the wild\cite {huang2007labeled} and YouTube faces\cite {wolf2011face} datasets.\relax }{figure.caption.99}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces An example of a one object in a training set.\relax }}{67}{figure.caption.101}}
\newlabel{fig:facenet-labeling}{{7.5}{67}{An example of a one object in a training set.\relax }{figure.caption.101}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces Accuracy during the facenet training.\relax }}{68}{figure.caption.105}}
\newlabel{fig:facenet_training}{{7.6}{68}{Accuracy during the facenet training.\relax }{figure.caption.105}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces Visualization of facenet embeddings using the T-SNE dimensionality reduction.\relax }}{68}{figure.caption.108}}
\newlabel{fig:facenet_training}{{7.8}{68}{Visualization of facenet embeddings using the T-SNE dimensionality reduction.\relax }{figure.caption.108}{}}
\newlabel{LastPage}{{}{68}{}{page.68}{}}
\xdef\lastpage@lastpage{68}
\xdef\lastpage@lastpageHy{68}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
