\pdfoutput=1
\documentclass[a4paper,11pt,titlepage,twoside]{article}
%\documentclass[a4paper,12pt,titlepage]{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amssymb,amsmath}
\usepackage{algorithm,algpseudocode}
\usepackage[title,titletoc]{appendix}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{url}
\numberwithin{figure}{section}
\usepackage{caption}
\usepackage{siunitx}
\usepackage{textcomp}
\usepackage[export]{adjustbox}
\usepackage{enumitem}


\newcommand{\Author}{Jan Bouček}
\newcommand{\Title}{Tracking vehicles across multiple non-overlapping fisheye cameras in a city environment}
\newcommand{\Acronym}{Acronym}
\newcommand{\WorkPackage}{WorkPackage}
\newcommand{\DocName}{Master thesis}
\newcommand{\Subject}{\WorkPackage - \DocName}
\newcommand{\Keywords}{Vehicle tracking}
\newcommand{\Date}{19/05/2018}
\newcommand{\DOCVersion}{0.1}
\newcommand{\jed}[1]{\ensuremath{~\mathrm{#1}}} %příkaz pro sazbu fyzikálních jednotek

\def\clinks{false}
\input{src/doc_head}

\renewcommand{\lstlistlistingname}{List of Algorithms}
\renewcommand{\lstlistingname}{Listing}
\definecolor{background_color}{rgb}{1.0, 1.0, 0.85}
\definecolor{comment_color}{rgb}{0.0, 0.5, 0.0}
\definecolor{keyword_color}{rgb}{0.0, 0.0, 1.0}
\definecolor{string_color}{rgb}{0.8, 0.0, 0.0}
\lstset{language=ksh}
\lstset{backgroundcolor=\color{background_color}}
\lstset{frameround=tttt}
\lstset{columns=fullflexible}
\lstset{keywordstyle=\color{keyword_color}\bfseries}
\lstset{commentstyle=\color{comment_color}}
\lstset{stringstyle=\color{string_color}}
\lstset{basicstyle=\ttfamily}
\lstset{showstringspaces=false}
\lstset{frame=single}
\lstset{keepspaces=true}
\lstset{tabsize=4}
\lstset{breaklines=true}
\lstset{captionpos=b}


\begin{document}

\input{src/title}
\pagestyle{empty}
\cleardoublepage

\input{src/prohlaseni}
\cleardoublepage

% zadani
\includepdf[pages={1}]{src/zadani.pdf}
\cleardoublepage

% Podekování
\input{src/podekovani}
\cleardoublepage

% Stránka s abstrakty
\input{src/abstract}
\input{src/abstrakt}
\cleardoublepage

% Záhlaví, římské číslování
\pagestyle{fancy}
\pagenumbering{roman}
\cfoot{\thepage}

% Obsah
\tableofcontents
\cleardoublepage

% Seznam obrázků
\listoffigures
\cleardoublepage

\pagestyle{fancy}
\pagenumbering{arabic}
\cfoot{}
\rfoot{\thepage$/$\pageref{LastPage}}
\setlength{\parskip}{0.35cm}

\lhead{\emph{\leftmark}}
\rhead{}

\section{Introduction}
Estimated 80\% of the world data is in form of images or videos \cite{cisco} and the percentage will likely increase. There is a lot of useful information hidden in videos, but it is very hard to extract it. The vast majority of the videos is processed manually by people, who make mistakes and are expensive. There is an incredible need for automated video processing in many branches of the industry for decreasing cost and for increasing speed and accuracy. Being able to accurately detect and track vehicles can provide valuable data about transportation to governments. Reidentification and tracking objects over multiple cameras in real time can help reinforcement agencies to effectively fight crime or surveillance agencies to prevent intrusion. 

The computer vision field has been experiencing an incredible advancement in the last couple of years thanks to the introduction of convolutional neural networks. They are successful on many problems from image classification to object detection and tracking. The state of the art in deep learning in computer vision was explored in this thesis.

This thesis was developed in cooperation with the company Good Vision s.r.o \cite{goodvision} for a law enforcement company from Brazil, to develop and deploy smart city solutions in South America. Good Vision provides a smart video analysis from street cameras , while the Brazilian partner provides the infrastructure. This thesis developed a multi-camera tracking of a vehicle in a city, that will be used by the police in many South American cities. 

When a crime is committed and a suspect drives away, there is a need for an automatic tracking of the vehicle. This is a difficult task and can easily fail when performed by people. This thesis introduces an automatic approach based on artificial intelligence and deep learning allowing fast and reliable tracking of a vehicle in a city.

The difference from standard setups is that the cameras are fisheye and they are mounted directly into street lamps above the vehicles as shown in the figure \ref{fig:stream3}. There are no public datasets for these kinds of images or videos and custom datasets had to be created from videos provided by the Brazilian partner.

For the final algorithm to be accurate, different subproblems had to be solved separately. That involves object detection, tracking, similarity, and reidentification as well as two datasets generation.

A deep learning object detector was introduced. It is based on the SSD \cite{liu2016ssd}, but utilizes the information from video by temporal difference and feeds it as an additional input layer. Experiments show, that the introduced approach achieves 91.6\% mAP on the presented domain, which is far better than the state of the art SSD network with just 63.2\% mAP when trained and tested on the same data.

Deep convolutional neural network Facenet \cite{schroff2015facenet} was successfully trained to recognize similar vehicles, which was used for reidentification a vehicle on a different camera. The overall algorithm was tested on a real-world scenario and can re-identify a vehicle with the 88 \% probability.

\subsection{Problem statement}

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{fig/stream3.png}
\caption{The fisheye camera is mounted in a lamp, therefore the view is directly from above.}
\label{fig:stream3}
\end{figure}

The smart city project is in development and only several cameras have been mounted. The cameras were built directly to street lamps and their streams are sent to a server. The cameras have a 360 degrees view thanks to their very short focal lengths and their locations in the city are known.

When there is a crime committed and the suspect is driving away in a vehicle, a camera operator marks the car and the goal of the project is to detect the vehicle, track its position in a single camera and be able to recognize it on different cameras. 

The camera resolution and optics don't allow using license plate recognition for reidentification.

\subsection{Overview of methodology}

The whole thesis was divided to subproblems and solved more or less separately. These solutions were connected into one for the final experiments.

\begin{itemize}

\item A thorough state of the art analysis was performed in the section \ref{sec:related_work} to be able to select the best approaches for different subproblems and to be able to compare developed solution to other approaches.

\item The camera parameters were not known and a mathematical model of the camera lens had to be created in section \ref{sec:lens} based on calibration data. This section also solves the transformations between the real world coordinates and their projection in the frame, which allows accurate localization of the vehicle.

\item The possibility of distributed computing directly in the cameras was explored as described in the section \ref{sec:classical}. That included fast set of algorithms for detection \cite{piccardi2004background}, tracking \cite{optical-flow} and classification \cite{haar} running on CPU. This approach could not be used directly because of a bad performance on high traffic scenes. However, it was used for a semi-supervised dataset generation and allowed training of a network for computing similarity between vehicles.

\item An annotation tool was used as described in the section \ref{sec:ssd} for creating training and validation dataset for object detection.

\item SSD \cite{liu2016ssd} neural network architecture was selected for vehicle detection because of its state of the art performance. It was extended for an additional input layer of temporal difference and additional feature layer to better recognize small moving objects and were trained on NVIDIA GeForce GTX 1080 for four days.

\item The original and the improved SSD neural networks were trained on the same dataset and their accuracies were compared in the section \ref{sec:ssd-evaluation}.

\item Google Facenet \cite{schroff2015facenet} was selected for computing similarities between vehicles and retrained on a custom dataset from section \ref{sec:classical}.

\item A city and vehicles representation was based on Markov chain and introduced in the section \ref{sec:city-representation}. It utilized the object similarity and relations between cameras in reidentification.

\item A real-world experiments were performed on cameras from a city and the implemented multi-camera tracking was tested and evaluated in the section \ref{sec:multi-camera-tracking}.

\end{itemize}

\subsection{Contribution}

Problems of various types were solved in this thesis, such as:

\begin{itemize}
\item An improved version of SSD \cite{liu2016ssd} was introduced and implemented, which achieved the mAP 91.6\% compared to the SSD from \cite{liu2016ssd} with the 63.2\% mAP on the wide-angle domain.

\item A dataset for vehicle detection on fisheye camera containing over 1600 images was created by standard annotating methods. Another dataset for training the similarity network was generated by an object detection and tracking algorithm containing over 9000 images.

\item The Facenet \cite{schroff2015facenet} network was retrained for vehicle similarity achieving the classification accuracy of 81\%.

\item A mathematical probabilistic representation of the multi-camera tracking problem was introduced based on Markov chains.

\item Multi-camera tracking experiments on real-world scenario were performed achieving the 88\% reidentification accuracy.

\end{itemize}

\clearpage
\section{Related work}
\label{sec:related_work}

The computer vision field has made an incredible leap forward in the last couple of years. Thanks to the increasing computational capabilities of computers and recent advancements in deep learning, we are able to do tasks, that we could not imagine. Image classification, location, and detection are tasks, that have gone through an incredible evolution in the last 5 years. The face recognition, autonomous driving, surveillance and many more fields have been the driving force for computer vision. The tracking of objects over multiple cameras is valuable in retail, traffic monitoring, and surveillance. 

\subsection{Classification}

Image classification is a task, where given an image, one class has to be assigned to the previously known set of classes. This is a hard task because of the variance in lighting, pose, rotation, scale, as well as intraclass variation. The detection task described in the section \ref{sec:detection} is linked to the classification problem, where the deep learning detectors use image classification networks.

Accuracy is measured as the proportion of correctly classified images in the test set. Two metrics are used. In top 1 accuracy, only one prediction is made. In top 5 accuracy, 5 predictions are made and an image is considered to be correctly classified if the correct class is among them.

To properly train, evaluate and compare models, several datasets, such as Mnist \cite{lecun-mnisthandwrittendigit-2010}, ImageNet \cite{deng2009imagenet}, or PASCAL VOC \cite{Everingham10} were created.

Before the invention of convolutional neural networks, other classifiers were used. Classifiers, in general, can be divided into parametric and nonparametric methods. The non-parametric ones require no training phase and the decision is based directly on the data. Most common method is the Nearest Neighbor approach \cite{boiman2008defense, zhang2006svm}. The parametric methods, on the other hand, require a training phase to find the parameters of the model, which can be in form of decision tree \cite{bosch2007image}, AdaBoost \cite{opelt2004weak}, or the most common Support Vector Machine (SVM).

The classification pipeline of SVM is such that a set of features is extracted into a vector and a SVM is applied. These features can have many different forms and can also be combined. A histogram \cite{chapelle1999support}, Bag of features \cite{lazebnik2006beyond, nowak2006sampling}, SIFT features \cite{yang2009linear, bicego2006use} or Haar features \cite{munder2006experimental} can be used.

Convolutional neural networks (CNN) are the state of the art in image classification. They were introduced in 1990's \cite{lecun1989backpropagation}, but only only since 2012 had a great success.

In 2012 AlexNet \cite{krizhevsky2012imagenet} won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) with the top-5 error being just a 15.4\%. This was a huge success compared to the second best with 26.2\% top-5 error rate and this is considered to be the beginning of deep learning in computer vision. 

The ZFNet \cite{zeiler2014visualizing} in 2103 introduced insides to how CNNs work by introducing deconvolutional network, that could show various feature activations. It also outperformed the AlexNet on ImageNet by the top-5 error rate being 14.8\% and winning the ILSVRC in 2013.

GoogLeNet/Inception won the ILSVRC 2014 with the incredible top-5 error rate of 6.67\%. The architecture was based on LeNet \cite{lecun1998gradient} but introduced an inception module. This module eliminates all full-connected layers, greatly reducing the number of parameters. It is used as a backbone in many object detection networks and was used in this thesis as a base network for Facenet \cite{schroff2015facenet} described in the section \ref{sec:facenet}

The second best network in ILSVRC 2014 was the VGG Net \cite{simonyan2014very}. The depth of the network was increased, but the number of parameters was kept low thanks to very small 3x3 filters. The architecture is simpler than of Inception and it is widely used as a detection network backbone. This network was used in this thesis as a backbone for the SSD \cite{liu2016ssd} described in the section \ref{sec:ssd}. 

The Microsoft's ResNet \cite{he2016deep} introduced a deeper architecture. They were able to train the network thanks to the introduction of the residual connections. Part of the information passes through each layer unchanged. This helps to solve the vanishing gradient problem \cite{hochreiter1998vanishing}. With the top-5 error rate 3.57\%, they surpassed the human accuracy winning the ILSVRC 2015.

The ResNet idea was further developed. Wide ResNet \cite{zagoruyko2016wide} reduced the number of layers while widened the network. ResNeXt \cite{xie2017aggregated} is furthermore highly modularized and introducing new dimension called cardinality, which is more effective, than simply increasing the number of layers or their width. 

DenseNet \cite{huang2017densely} connects each layer to all previous layers. This furthermore helps with the vanishing gradient problem and reducing the number of parameters. It outperforms ResNet while requiring less memory and computation.

The task of image classification is considered to be solved, but more research is being done. These image classification networks can be used as a backbone for other tasks, such as image detection, localization or segmentation.

\subsection{Object Detection}
To be able to track vehicles, they need to be first detected. The most common methods use object detection from cameras. This section introduces a general vehicle detection as well as general object detection in images.

\label{sec:detection}
\subsubsection{Vehicle detection}
There are many ways how to detect vehicles, not just with cameras. One can detect changes in magnetic fields \cite{daubaras2012vehicle, caruso1999vehicle} or use a laser scanner \cite{gate2009fast}.

Cameras are the most common sensor, but they can be also combined with a laser scanner \cite{wender20083d, premebida2007lidar} or a sonar \cite{kim2005front, wang2003online}. Sometimes a stereo vision \cite{bertozzi2000stereo, toulminet2006vehicle} can be used to gain a better model of the environment.

A lot of research was done for detection of vehicles and pedestrians thanks to the recent advancements in autonomous driving. Many datasets were created \cite{huang2018apolloscape, madhavan2017bdd, RobotCarDatasetIJRR, ncarlevaris-2015a} for detecting vehicles, pedestrians and other objects from the vehicle point of view. There is even a research for detecting vehicles by their shadow \cite{tzomakas1998vehicle}. The state of the art in vehicle detection using cameras is detecting each image independently using techniques described in the section \ref{sec:object-tracking}.


\subsubsection{Object detection in computer vision}
\label{sec:object_detection}
In computer vision, the object detection is a specified task. The goal is to draw a rectangle (bounding box) around each object and classify it. The accuracy is measured in mean Average Precision described in the section \ref{sec:mAP}. 

Before the introduction of neural networks, various methods were used for object detection. Haar features were used for detecting faces \cite{haar, lienhart2002extended, viola2004robust} and vehicles \cite{sun2002real}. For general object detection, the background subtraction \cite{piccardi2004background, horprasert1999statistical} or optical flow \cite{naoya1990optical, quenot1992orthogonal, chen2011tracking} described in the sections \ref{sec:bgs} and \ref{sec:optical-flow}. SIFT \cite{lowe2004distinctive} HOG \cite{girshick2014rich, wang2009hog, zhu2006fast, felzenszwalb2010object, dalal2005histograms} were also used.

The big advancements came with the introduction of region proposal networks \cite{girshick2016region}. R-CNN \cite{girshick2014rich} was the first to introduce this concept. It consists of two neural networks, one to propose the regions of interests and the second one to classify them. Their performance was mAP of 53.3\% on PASCAL VOC 2012 dataset. This was a huge success compared to the mAP of 43.3 \%\cite{carreira2012cpmc} the year before. However, R-CNN was very slow (47 seconds to detect an image on GPU with the VGG16 \cite{simonyan2014very} network), thus were far from real-time video analysis. It requires a full convolutional network forward pass for each of the around 2000 proposals.

Improved and faster version Fast R-CNN \cite{girshick2015fast} achieved 68.4\% on PASCAL VOC 2012 with the VGG16 network while significantly increasing speed over 200 times compared to R-CNN. This was due to sharing computations over proposals and using a single network for the feature extractor, classifier and the regressor in one network. However, the selective search for the region proposals was found to be the bottleneck for the detection process.

The Faster R-CNN focused on exactly that. The feature extractor was also used for the region proposal network making the region proposal almost cost-free. They also increased the learning speed, because only one CNN needed to be trained. Faster R-CNN with VGG16 achieved 75.9\% mAP on PASCAL VOC 2012 dataset with just 7 fps on GPU. This is much closer to processing a real-time video.

The YOLO \cite{redmon2016you} performs 45 fps while achieving 63.4 mAP on VOC 2007. It splits the image in a grid and predicts only two bounding boxes and class probabilities for a square. However, it struggles with detecting more small objects close to each other and would not be a good detector for vehicles from the street camera. However, there were some improvements to this network \cite{redmon2017yolo9000, redmon2018yolov3}.

Region convolutional neural network, which create proposals and then classify them, are still too slow. The Single shot multibox detector (SSD) \cite{liu2016ssd} based on \cite{erhan2014scalable} leaves out the region proposals completely and has a fixed number of regions. It was introduced in November 2016 and had an incredible 74.3\% mAP at 59 fps on VOC 2007. This network was chosen for object detection in this thesis and was described more in detail the section \ref{sec:ssd}. 

\begin{table}
\centering
\begin{tabular}{|l|c|c|c|c|c|}
  \hline
  &Faster R-CNN & Fast YOLO & YOLO & SSD300 & SSD512 \\
  \hline
  fps & 7 & 155 & 21 & 59 & 22\\
  \hline
  mAP & 73.2 & 52.7 & 66.4 & 74.3 & 76.8\\
  \hline
\end{tabular}
\caption{Results on PASCAL VOC2007 test.}
\end{table}

There were lately many more architectures introduced, such as \cite{lin2017focal, li2017fssd, dai2016r} and many more are coming.

These networks process images, but are used also for detecting video. Most common schema of detection objects in a video is, that the video is decomposed to different frames and each frame is detected independently \cite{sudowe2011efficient}. This loses a lot of the information encoded in the video. Background subtraction \cite{gonzalez2012digital} or optical flow \cite{naoya1990optical} as described in the section \ref{sec:classical} can be used for extracting more information from the video context. 

Optical flow can be connected with a neural network \cite{ng2015beyond}, but the optical flow is expensive to compute, even though there is a convolutional network for optical flow estimation \cite{simonyan2014two}. \cite{karpathy2014large} preserves the video information by taking as an input multiple frames from the video, but this makes the model large. 

A good trade-off between the network's size and preserving video information was introduced in this thesis by combining the RGB input image with the 4th channel of temporal difference between frames and feeding it to a neural network. There is a research \cite{bayona2010stationary, koprinska2001temporal} using the temporal difference for detection and segmentation, but to my knowledge was not combined with deep learning. To our best knowledge, there was no published result of designing a detector capable of processing fisheye distorted images. 

\subsection{Object tracking}
\label{sec:object-tracking}

The previously described tasks process single images. Now the problem expands to a new discrete-time dimension when processing video, but for now, keeping just one video feed. The goal is to create a trajectory or a sequence of bounding boxes of an object. This task is difficult because of the changes in illumination, partial and full object occlusions and the real-time processing requirements \cite{yilmaz2006object}. Almost all trackers assume, that the frame rate of the video is high enough, that the movements of the objects are smooth. The approach can be divided into a dense and a sparse method. 

The sparse method scans only pixels nearby the tracked objects and tries to estimate their movement. The input is a position of the object and it is tracked over upcoming frames. This is especially good for tracking one object. This method has not been chosen, since there is a need for continuous detection of incoming vehicles and the dense method has been used. The object can be represented as a single point \cite{kale2015moving}, a bounding box \cite{comaniciu2003kernel, porikli2005multi, yilmaz2007object, elgammal2002background}, or a silhouette \cite{isard2001bramble}. Only the changes can be registered \cite{kale2015moving} or a robust reidentification \cite{veenman1998fast} can be used, which performs better with occlusions, than standard methods. A statistical representation can be connected with a Kalman filter \cite{banerjee2008multi} or a particle filter \cite{zhong2012moving}. The movement is often estimated using sparse optical flow \cite{kale2015moving, mae1996object}. With the recent deep learning advancements, tasks as tracking are also being solved with deep neural networks \cite{bertinetto2016fully, held2016learning, gladh2016deep, gaidon2016virtual, lee2016globally}.

The dense methods for tracking receives a video and detections for each frame. This has the advantage, that the tracks can be created without explicitly manually selecting each object we want to track. This method has been chosen, since the vehicle must be detected from each camera. However, these approaches are more computationally complex, since they require object detection. The tracker clusters the bounding boxes into tracks. The main methods use Jaccard overlap \cite{tan2005introduction, benfold2011stable} and optical flow \cite{chen2011tracking}. This task can be complex because of the crossing tracks as well as false positive and false negative detections \cite{joshi2012survey, elgammal2002background}.

\subsection{Reidentification}
When an object leaves one camera and appears in another camera, the task is to recognize it. When positions and orientations of the cameras are not known, the location and speed of the detected objects can be used for obtaining the spatial relationships among the cameras \cite{makris2004bridging}. The key to reliable reidentification is to correctly model the relationships among the cameras, as well as to find a similarity metrics of the detected objects. When the cameras overlap, the key is to accurately estimate the position of the tracked object and match them \cite{khan2003consistent, krumm2000multi, zhao2005real}. The detected objects can look very differently on different scenes because of the different scaling, rotations and lighting conditions. The brightness transfer function can be estimated and compensated \cite{javed2005appearance, porikli2003inter}. 

When the camera fields of view don't overlap, the task becomes much more challenging. The camera positions can be either known \cite{rahimi2004simultaneous} or unknown \cite{makris2004bridging}. For reidentification, mean a posteriori (MAP) is estimated, giving the probability of the detected object being the same \cite{javed2005appearance, huang1997object}. \cite{kettnaker1999bayesian} used a probabilistic Bayesian model formulation with previously known transition functions. \cite{kang2003continuous} explored this system for controllable movable cameras. 


The state of the art multi-camera vehicle tracking approaches are very domain specific. They are either set for highways with hard-coded lanes \cite{coifman2007vehicle, kuhne1991freeway}, rely on license plate recognition \cite{arth2007real, du2013automatic}, have very narrow field of view \cite{matei2011vehicle} or even use magnetic sensors \cite{kwong2009arterial}. 


The methods rely on the same schema: object detection and tracking on cameras and matching these tracks using similarity and a prior knowledge about relations between the cameras. The similarity of vehicles is usually decided by the license plate recognition as mentioned before. The cameras used in the thesis are not good enough to recognize a license plate. There is a classification network for identifying a car's model, but they require a good quality image of the front of the car \cite{munroe2005multi}. Since similarity between vehicles was not otherwise explored, a similarity of faces was.

The network Facenet \cite{schroff2015facenet} by Google is the state of the art for deciding similarity of faces. It is built on powerful Google inception \cite{szegedy2016rethinking} network. It achieved 99.63\% accuracy on the Labeled Faces in the Wild \cite{huang2007labeled} dataset and 95.12\% accuracy on YouTube Faces \cite{wolf2011face} dataset. This network was selected and retrained in this thesis for vehicle similarity. 


\clearpage
\section{Fisheye camera model}
\label{sec:lens}

\begin{figure}[h]
\centering
\includegraphics[width=1\linewidth]{fig/stream1.png}
\caption{Frame of the provided video.}
\label{fig:stream1}
\end{figure}

For correct estimation of position of detected objects, it is crucial to find the relationships between the camera pixel position and the real world positions. 

The cameras were provided by the Brazilian party and no technical parameters were available. The model of the camera and its parameters had to be computed. A set of improvised requested calibration images were provided shown in the \ref{fig:calibration}.

\subsection{Scene localization}
\label{sec:scene_localization}
Before selecting the model of the cameras, another hardware error of the camera had to be compensated for. As can be seen in the image \ref{fig:stream1}, the scene is shifted in the frame to the left down. It is not even circle, but rather an ellipse. This is due to manufacturing uncertainty of sensor placement and this error is different in each camera. Since this project has to be easily scalable when adding new cameras, and it is not convenient to measure and set the parameters manually, an universal algorithm for detecting ellipse was developed in this thesis.

The algorithm, which was designed in this thesis, is based on an iterative optimization. It takes an image as an input and produces parameters of the ellipse. From observation, the ellipse can only be either the horizontal major axis or the vertical major axis ellipse. The equation \ref{eq:ellipse} of the ellipse is rather unusual, but this formulation allows faster cost function evaluation.

\begin{equation}
\label{eq:ellipse}
\frac{(x-s_x)^2}{a} + \frac{(y-s_y)^2}{1} = r^2
\end{equation}

Now we need to find the parameters $s_x, s_y, a, r$.

The original image $I$ of the size $H, W$, and channels $I_1, I_2, I_3$ is transformed to a mask $M$ of the same size by thresholding the total sum of channels on the 8-bit scale is greater or equal to 1. 

\begin{equation}
M_{x,y} = \begin{cases}
1 & if \quad \sum_{i=1}^{3} I_{i,x,y} \geq 1 \\
0 & otherwise
\end{cases}
\end{equation}

The mask $M$ represents the scene by the pixels with the value 1 and the background by the pixels with the value 0. 

We create a predicted mask $E(s_x, s_y, a, r)$ of the ellipse as 

\begin{equation}
E_{x,y}(s_x, s_y, a, r) = \begin{cases}
1 & if \quad \frac{(x-s_x)^2}{a} + \frac{(y-s_y)^2}{1} \leq r^2 \\
0 & otherwise
\end{cases}
\end{equation}

The cost function $C(M, E(s_x, s_y, a, r))$ penalizes the pixels that were masked as the scene and lie outside the ellipse and the pixels, that were masked as background and lie inside the ellipse.

\begin{equation}
C(M, s_x, s_y, a, r) = \sum_{x = 0}^{W-1} \sum_{y = 0}^{H-1} E_{x,y}(s_x, s_y, a, r) \cdot (1-M_{x,y}) + (1 - E_{x,y}(s_x, s_y, a, r)) \cdot M_{x,y}
\end{equation}

The algorithm could evaluate all combinations of parameters, but the number of searched parameters can be greatly reduced by searching in a coarse to fine manner. 

In each step, a baseline is set and for each parameter, a higher and a lower value by a constant is evaluated. The best value is selected and set as a new baseline for the next step and the constant is divided by two. The main idea is based on a binary search.

The cost function evaluations can be run in parallel, which can speed up the process on multi-core CPU.

\subsection{Camera model}

To correctly localize object from the camera, we need to know the transformations between real-world coordinates $x^w, y^w, z^w$ and the projection on the captured frame $x^f, y^f$. After applying the algorithm from \ref{sec:scene_localization}, we know, where in the frame the scene is projected. First, we will consider the circle model and at the end, we will apply the transformation to the ellipse. 

Computing in the cartesian coordinates is not very useful for optics, because the view of a camera is inside a cone. Instead, the world coordinates are chosen to be spherical and the frame coordinates are chosen to be polar. The world coordinates are with respect to the camera. The transformations between the world cartesian coordinates $x^w, y^w, x^w$ and the world spherical coordinates $r^w, \theta^w, \phi^w$ are standard transformation equations for spherical coordinates:

\begin{figure}[h]
\centering
\includegraphics[width=0.5\linewidth]{fig/sphere.png}
\caption{The spherical coordinates of the world.}
\label{fig:sphere}
\end{figure}


\begin{equation}
\begin{aligned}[c]
x^w &= r^w \cdot cos(\theta^w) \cdot cos(\varphi^w) \\
y^w &= r^w \cdot cos(\theta^w) \cdot sin(\varphi^w) \\
z^w &= r^w \cdot sin(\theta^w) \\
\end{aligned}
\qquad,\qquad
\begin{aligned}[c]
r^w &= \sqrt{(x^w)^2 + (y^w)^2 + (z^w)^2} \\
\theta^w &= arcsin\Big(\frac{z^w}{r^w}\Big) \\
\varphi^w &= arctan\Big(\frac{y^w}{x^w}\Big) \\
\end{aligned}
\end{equation}



The detected scene circle has the radius of $R$ pixels and the center at pixels $s_x, s_y$. The transformations between cartesian frame coordinates $x^f, y^f$ and the polar frame coordinates $r^f, \theta^f$ are:

\begin{equation}
\begin{aligned}[c]
x^f &= s_x + R \cdot r^f \cdot cos(\varphi^f) \\
y^f &= s_y + R \cdot r^f \cdot sin(\varphi^f) \\
\end{aligned}
\qquad,\qquad
\begin{aligned}[c]
r^f &= \sqrt{(x^f - s_x)^2 + (y^f - s_y)^2} \\
\varphi^f &= arctan\big{(}\frac{y^f - s_y}{x^f - s_x}\big{)}
\end{aligned}
\end{equation}

\begin{figure}[h!]
\centering
\includegraphics[width=0.7\linewidth]{fig/calibration.png}
\caption{The provided calibration data.}
\label{fig:calibration}
\end{figure}

The transformations between the world spherical coordinates $r^w, \theta^w, \phi^w$ and the frame polar coordinates $r^f, \theta^f$ need to be found. However, there are some nice properties:

\begin{itemize}
\item The $\varphi$ are the same, i.e. $\varphi^w = \varphi^f$. That results from aligning both coordinate systems. 
\item The transformations do not depend on $r^w$. The projection depends only on the direction, not on the distance from the camera.
\end{itemize}

With this knowledge, only the he transformation of $\theta^w$ and $r^f$ needed to be found by representation of function $f$, such that 
\begin{equation}
\begin{aligned}
\theta^w &= f(r^f) \\
r^f &= f^{-1}(\theta^w). \\
\end{aligned}
\end{equation}

There are many models for finding $f$ \cite{courbon2007generic}. 

\begin{itemize}
\item The linear model: $f(r^f) = FOV \cdot r^f$
\item The tangent model: $f(r^f) = FOV \cdot \tan(r^f)$
\item The sinus model: $f(r^f) = FOV \cdot \sin(r^f)$
\end{itemize}

With each model, the only parameter, that had to be found was $FOV$, which is the field of view of the camera.

There was no access to the cameras, so the standard calibration using mesh could not be used. Instead, a set of marks was provided as shown in the figure \ref{fig:calibration}. These marks are exactly 2 meters apart and are enough to estimate the function $f(r^f)$.

\subsubsection{Linear model}

This is the simplest model. The real world angle is proportional to the distance from the center on the image. 

\begin{equation}
\theta^w = f(r^f) = FOV \cdot r^f
\end{equation}

Fitting of the model is shown in the fig.\ref{fig:linear_model}

\begin{figure}
    \begin{subfigure}[Sample1]{0.5\linewidth}
        \includegraphics[height=60mm]{fig/linear_model3.eps}
        \caption{The linear model of the lens.}
        \label{fig:linear_model}
    \end{subfigure}
    \qquad
    \begin{subfigure}[Sample1]{0.5\linewidth}    
        \includegraphics[height=60mm]{fig/tan_model2.eps}  
        \caption{The tangent model of the lens.}
        \label{fig:tan_model}
    \end{subfigure} 
    \caption{Approximation of the calibration data by a linear and tangent model of the lens.}
\end{figure}


As seen in the figure \ref{fig:linear_model}, the linear model estimates the real one well only for small angles.

\subsubsection{Tangent model}

This is a more complicated model, which is based on the pinhole camera model.

\begin{equation}
\theta^w = f(r^f) = \theta^w \cdot FOV,
\end{equation}

Fitting of the model is shown in the fig.\ref{fig:linear_model}. This model represents the camera optics much and was chosen to be the final one.

\subsection{The city coordinate system}

There are many ways how to represent a vehicle in a city. The most obvious one would represent the position by longitude, latitude, and elevation. This would be correct, but not very practical. Since the distances between same circles of latitude and longitude are different, this would need more complicated transformations and there is a simpler model. 

Since we care only about only one city, we will use a city cartesian coordinate system $(x^c, y^c)$. We can choose any position and rotation of the coordinate system. All we need to know is the relative translations and rotations of each camera $(\Delta x, \Delta y, \Delta\phi)$ to the city coordinate system. For simple transformations, we will use homogeneous coordinates \cite{riesenfeld1981homogeneous}, which are in form of $(x, y, 1)^T$. Any transformation of coordinate systems can be represented as a matrix multiplication.

A point from a camera coordinate system $(x^w, y^w)$ is transformed to the city coordinate system $(x^c, y^c)$ by a standard 2D rotation and translation matrix \cite{riesenfeld1981homogeneous} as
\[
  \begin{bmatrix}
    x^c \\
    y^c \\
    1
  \end{bmatrix}
   = 
  \begin{bmatrix}
    cos\Delta\phi & -sin\Delta\phi & \Delta x\\
    sin\Delta\phi & cos\Delta\phi & \Delta y\\
    0 & 0 & 1
  \end{bmatrix}   
  \cdot 
  \begin{bmatrix}
    x^w \\
    y^w \\
    1
  \end{bmatrix}
\]







\clearpage
\section{Dataset generation}
\label{sec:classical}

A detection and tracking of vehicles without using deep learning was explored. For several reasons described in this section, it was not used for the final product, but allowed a much faster video annotating, than standard methods and was used to create the dataset for training similarity between vehicles described in the section \ref{sec:similarity}


\subsection{Need for a custom dataset}

There are publicly available datasets for image classification and object detection, such as ImageNet \cite{deng2009imagenet}, COCO \cite{lin2014microsoft}. They also contain images of cars, people, trucks, and other classes. However, The section \ref{sec:mask-rcnn} shows, that these datasets cannot be used on their own because of the angle, lens type, and other factors, that differ from our cameras.

\begin{figure}[h!]
    \begin{subfigure}[Sample1]{0.5\linewidth}
        \includegraphics[height=50mm]{fig/coco0.jpg}
        \caption{Example of COCO photo for object detection.}
        \label{fig:coco0}
    \end{subfigure}
    \qquad
    \begin{subfigure}[Sample1]{0.5\linewidth}    
        \includegraphics[height=50mm]{fig/imagenet3.jpg}  
        \caption{Example of ImageNet photo for image classification.}
        \label{fig:imagenet3}  
    \end{subfigure} 
    \caption{The car class examples in ImageNet \cite{deng2009imagenet} and COCO \cite{lin2014microsoft}.}
\end{figure}

In our domain vehicles viewed from directly above or are very distorted. Creating dataset from provided videos is the only way to train object detection network. 

\subsection{Distributed system}
This section, apart from dataset generation, introduces a computationally fast solution capable of running inside lamps, if they were equipped with CPU. 

The final system needs to be easily scalable with additional cameras and a particular system architecture was explored. If the detections and tracking were computed on-board of the cameras, that would help greatly. There would be much less communication needed. Instead of transferring whole video streams, only some meta-data would be sent. That would include: 

\begin{itemize}
\item Time stamps of frames.
\item Locations of objects and their classes.
\item Some description vector of the detections.
\item Detections clustered to tracks.
\end{itemize}

This system could be greatly distributed sending packets of information among only the cameras that the information is relevant to.

However, this has some downfalls, mainly in the computational manner. Each camera would have to be equipped either with a capable computational unit. The detections, tracking, and similarities would all have to be computed onboard. Since it is not possible to have a GPU in every lamp for many reasons, for example, it is a very wet environment, usage of neural networks would not be possible. This section introduces approach for detection, tracking and classification, that could run on CPU inside lamp.

\subsection{Background subtraction detection}
\label{sec:bgs}

Probably the best classical detection methods from static videos, that can be computed in real-time on limited hardware, are based on the background subtraction algorithm \cite{piccardi2004background}. The main idea is creating a model of the scene without the objects that we want to detect and then subtracting the current frame and by thresholding determine, where the vehicles are. This simple approach showed many false positives and some improvements need to be made. 

For the background subtraction procedure, a model of the background had to be found. For our purposes, an model needs to be known of how the road looks like without any vehicles and people. This can not be done by simply waiting for such a case, but the traffic is usually high. Instead, the background needs to be acquired from multiple images. 

The algorithm was implemented in OpenCV \cite{opencv}. The background is usually created by the mean over several images called running gaussian average \cite{wren1997pfinder}. The idea is to estimate a Gaussian to each pixel independently. Each pixel is updated with each new frame as a weighted sum. The background looks like a photo with a long exposition. In places, where vehicles drive, are colored lines as shown in the figure \ref{fig:cut_mean}. When computed the difference from a video frame to such a background model, as shown in the figure \ref{fig:threshold_mean}, the places, where usually cars drive, can have high values. This can be bad for creating a mask by thresholding because a higher thresholding constant has to be set.

\begin{figure}
    \begin{subfigure}[Sample1]{0.5\linewidth}
        \includegraphics[height=37mm]{fig/background_mean_crop.png}
        \caption{Mean model.}
        \label{fig:cut_mean}
    \end{subfigure}
    \qquad
    \begin{subfigure}[Sample1]{0.5\linewidth}    
        \includegraphics[height=37mm]{fig/background_med_crop.png}  
        \caption{Median model.}
        \label{fig:cut_med}  
    \end{subfigure} 
    \caption{Background model created by the mean and the median approach.}
    \label{fig:roundabout}
\end{figure}

The background model changes with every new frame. In the first iteration, the background $B_0$ is the first frame $F_0$. The background in next iteration is the weighted sum of the  current frame and the background model in the previous iteration.

\begin{equation}
B_n = \alpha \cdot F_n + (1 - \alpha) \cdot B_{n-1}
\end{equation}

This algorithm is simple, fast and can be highly parallelized. The picture having $N$  pixels, the complexity of this standard background subtraction algorithm is  $O(N)$.



\begin{figure}
    \begin{subfigure}[Sample1]{0.5\linewidth}
        \includegraphics[height=37mm]{fig/threshold_mean_crop.png}
        \caption{Mean.}
        \label{fig:threshold_mean}
    \end{subfigure}
    \qquad
    \begin{subfigure}[Sample1]{0.5\linewidth}    
        \includegraphics[height=37mm]{fig/threshold_med_crop.png}
        \caption{Median.}
        \label{fig:threshold_med}    
    \end{subfigure} 
    \caption{The difference between the frame and a background shown in a gray-scale.}
\end{figure}

An improved way of acquiring background model was introduced, which greatly improves the quality of current background subtraction methods. A simple change of taking the median instead of the mean at each pixel position gives a much better estimation of the background \cite{bgs-med1, bgs-med2}. The algorithm keeps a queue of $K$ images in a memory and with each incoming frame it puts it in the database and for each pixel, it computes a median from the queue. This algorithm can be implemented with the complexity $O(N \cdot log(K))$ if we insert each pixel from an incoming frame to a sorted structure. In reality, for small $K$ this would slow the algorithm because in OpenCV and numpy there is a great support for working with the whole images. This approach simply computes the median over all the images from the queue. The complexity is $O(N \cdot K \cdot log(K)).$ The $K$ was set to 35. The histogram of a particular pixel position over the queue is shown in the figure \ref{fig:pixel_hist}.


This turns out to work much better, but still has its limits. If the traffic is very high and vehicles occupy on average more than half of the ground, the background model will fail, but mean approach would fail as well. 

\begin{figure}[h]
\centering
\includegraphics[width=1\linewidth]{fig/pixel_hist.eps}
\caption{The histogram of a particular pixel over 100 images with computed medians from the scene in figure \ref{fig:roundabout}.}
\label{fig:pixel_hist}
\end{figure}

Some more complicated models based on unsupervised learning, such as clustering or taking the most frequent bin from histogram for each pixel position could be used, but they would be computationally too complex and would not be practical for real-time video.

The difference between background model and the current frame is very noisy and some filtration had to be made. Before subtraction, the background and the frame were filtrated with a Gaussian filter with the size 3x3 for smoothing. This compensates for the camera vibrations. Then smoothed again with the filter 11x11. This serves as an apriori probability. The idea is, that if there are big differences in the neighboring area, it is a higher probability of the pixel belonging to the car. This also helps to detect gray and black cars, which have a similar color to the road. Another advantage is, that this greatly reduces noise and helps to detect vehicles as a whole.

This differential image is thresholded and a mask is obtained as shown in the figure \ref{fig:mask_area}. Each blob is presented with a contour and they are thresholded once more by the area. The resulted blobs become detections and a bounding box is created.

\begin{figure}
    \begin{subfigure}[Sample1]{0.5\linewidth}
    	\includegraphics[height=80mm]{fig/frame_cropped.png} 
        \caption{A frame for detection.}
        \label{fig:frame_for_detection}   
    \end{subfigure}
    \qquad
    \begin{subfigure}[Sample1]{0.5\linewidth} 
    	\includegraphics[height=80mm]{fig/mask_area.png}
        \caption{The detections and areas of contours.}   
        \label{fig:mask_area}
    \end{subfigure} 
    \caption{The background subtraction detection algorithm.}
\end{figure}

The background must be continuously adapting to the scene, but the rate of adapting is crucial. If the background is changing too slowly, it will create false positives with changes of lightning from coming clouds, etc. If the background adapts too fast, it will start to contain cars, that stop at the cross section and when the cars leave, this will become a new false detection. From experiments, there is no optimal adapting rate and it depends on the scene, weather and even then these problems will not completely disappear. One small advantage is filtering detections while adapting background.

Background subtraction is a computationally fast detection algorithm and when having perfect conditions, generated bounding boxes are accurate. Unfortunately, it has many downsides:

\begin{itemize}
\item Moving trees and their shadows create false positives.
\item Overlapping vehicles are detected as only one.
\item It works badly in high traffic because it can't create a correct background model.
\item It is very sensitive to changes of lightning, such as moving clouds.
\item It is very sensitive to setting of hyper-parameters.
\end{itemize}

Most of these points relate to changing the background. Especially if the scene is partly cloudy and the lighting changes a lot, the background model needs to adapt quickly. On the other hand, if in the scene is a traffic light, cars spend a lot of time on one spot and could be incorporated to the background model. Not only the car will not be detected, but a false positive will be detected when the car leaves.

For these problems, background subtraction alone can't be used as a good detector, but on perfect scenes, it can be very useful for collecting high-quality training data for neural networks detectors, as described in the section \ref{sec:ssd-implementation}.

\subsection{Optical Flow tracking}
\label{sec:optical-flow}
The detections were described in section \ref{sec:bgs}. Having only the detections for each frame does not give us that much information. We need to connect these detections to a track.

A custom set of algorithms was implemented in opencv\cite{opencv} for extending the background subtraction algorithm to track. Optical flow \cite{optical-flow} is used for a motion estimation in a video. It pairs pixels in two subsequent frames. In other words, it is a discrete 2D vector field, where each vector is a displacement vector showing the movement of points from the first frame to second.

The computation of optical flow over the whole image is usually a very expensive procedure. The method used is Lucas-Kanede method \cite{lucas-kanede}. 

This computation is not used on the whole image. That would be computationally too expensive and would not be feasible for limited computational resources and real-time system. Instead, each detection is extended for a one optical flow point. In the next frame, this point will move with the object. Each detection is characterized by this point. 

This extends the detector for object tracking and partially solves the problem of two overlapping vehicles. If two vehicles drive close to each other, background subtraction would start to treat them as one object. This improved model will detect this situation and try to keep the bounding boxes on the different vehicles. 

In first experiments, when a new detection appeared, the position of the optical flow point was selected with the Shi-Tomasi \cite{shi-tomasi} algorithm, which is an improved version of the Harris corner and edge detector \cite{harris}. It tries to find some features, that have a high gradient and will be easier to track, rather than selecting just the middle of the bounding box.

In a perfect scenario, the algorithm could be used without further improvements. However, in real-world scenario, false positives, as well as false negatives detections must be dealt with. Furthermore, trees and lamps also complicate the situation greatly. When a vehicle drives behind some sort of a pillar, the optical flow point cannot be matched with a next frame and stays in the same place in the frame. When the vehicle completely passes, the tracking is lost. A feature had to be added, which is an additional centering of the optical flow point to the middle of the bounding box. With each new frame, the optical flow point moves with the vehicle, but is also moved towards the middle of the bounding box. This solves the issues of the pillar obstacles but excludes using the Shi-Tomasi and Harris features. 


The tracking algorithm is described by a set of rules. The main ones are:
\begin{itemize}
    \item If an optical flow point is outside the background subtraction mask, it becomes a 'zombie'.
    \item If a background subtraction detection is without an optical flow point and there is no 'zombie' in the detection, optical flow point is created in the middle of the bounding box. 
    \item If a zombie is not recovered in 10 frames, it disappears. 
    \item If there are more optical flow points in one background subtraction detection, the bounding boxes continue moving with a low pass filter.
    \item The optical flow points are forced to the middle of the bounding box. This solves the problem of the optical flow points being created on the front of an incoming vehicle. When the vehicle is seen from a side, the point is on the edge of bounding box and due to noise can move outside. 
\end{itemize}

These improvements work surprisingly well and solve the tracking problem to some extent. If vehicles overlap completely, this approach will fail, but so will most of the other ones. 

\subsection{Classification}
There are many objects detected in a frame and need to be classified. The most important classes are a person and a vehicle. If a classification was accurate, simple scenes could be annotated 100\% automatically. That would mean gaining labeled data almost for free and creating huge amounts of datasets for object detection networks.

A simple classifier was introduced using Haar features \cite{haar} and the SVM classifier. Each Haar feature is a difference of average brightness level in two rectangular areas in the image. 87 distinct haar features were extracted and a vector of real numbers was created representing the image. This vector was the input into an SVM classifier.

The SVM classifier was trained on 8144 images of cars and 10567 images of people. This dataset was extracted from the fisheye traffic videos provided to us, using the methods described in the section \ref{sec:data-generation}. The test set split ratio was 0.2.

Accuracy on the test set was 0.842. That is quite a good score without using neural networks, but still too far from deploying on the final project or dataset creation and it was not used.

\subsection{Semi-supervised dataset generation} 
\label{sec:data-generation}

\begin{figure}[h!]
\centering
\includegraphics[width=0.5\linewidth]{fig/omni_labeling.png}
\caption{The dialog from the annotation tool.}
\label{fig:labeling}
\end{figure}

Standard annotation approaches for image detection training need the annotator to draw a rectangle and assign it a class for every detection. This is very time consuming and therefore costly. A video is usually around 25 frames per second, so to annotate a minute of video means annotate 1500 images. This process can be made easier by skipping some frames and moving the bounding boxes around. For the skipped frames linearly approximate the movement of the objects. This can speed up the process, but it still takes a very long time. 


The algorithms introduced in this section cannot be used for the final product, mainly because of the problems described in the section \ref{sec:bgs}. However, that does not mean, that this can not be used for other purposes. As mentioned before, it works very well on easy scenes. The bounding boxes are precise and this can help the annotator to annotate scenes faster and more precisely, than standard approaches. the annotator is presented with a dialog as shown in the figure \ref{fig:labeling}. That includes the whole track, which is annotated by a user through one click. 


Because of the not perfect classifier, classification is done by the user. The annotator manually selects the class of the detected track.


\clearpage
\section{Convolutional Neural Networks}

Deep learning is used for artificial intelligence models, which use more non-linear layers of computation. Most common are the artificial neural networks. They are a computational model inspired by a brain, that can be thought various tasks from image classification \cite{szegedy2015going} to speech processing \cite{graves2013speech}. In this section, we will focus on convolutional neural networks for image processing.

\subsection{Inspiration by biology}
Brains are incredibly capable of processing images. Half of a human brain is either directly or indirectly devoted to vision \cite{mit-vision}. Vision can also be a very parallelized process, which is how a brain is structured.

The basic computational unit in a brain is a neuron. A neuron in a brain has an input and an output. The input is a dendritic tree, which is connected to outputs(axons) of other neurons. Neurons are only unidirectional and their output is binary. They either fire, if the input is strong enough or they don't. Their connections to other neurons can vary from very weak to a very strong one and their size can change by learning.

The basic element of artificial neural networks is also a neuron. It also has several inputs and one output. The most common neuron can be also called a perceptron\cite{rosenblatt1958perceptron}, which is a well-known classifier. The basic function of a neuron can be described as $f(\bold{\omega} \cdot \bold{x} + b)$, where $\bold{x}$ is the input of the neuron, $\bold{\omega}$ is the output and $f$ is the activation function and $b$ is the bias. The weights and the biases are the only thing, that changes during the training, where the goal is to find such weights and biases, that the neural networks perform the required task well.

When an architecture is created, the neural network needs to learn how to perform the required task. It figures that out in a training phase from showing the network many input data with labels, for example, many pictures and classes of the objects in the images. This is called a supervised learning. After the training phase, the weights are frozen and the network can perform the task, that is was trained to do.

While the structure of the neural network is inspired by the brain, the training is not. The way it is performed is explained more in detail in the section \ref{sec:back-prop}.

\subsection{Layers}
Neurons in artificial neural networks, similarly like in a brain, are organized in layers. These layers are usually connected to each other in a serial way. Neurons in one layer perform the same function. There are more types of layers depending on the neuron function and the way they are connected. The information describing this defines the neural network architecture.


\subsubsection{Convolutional layer}

\begin{figure}[h!]
    \begin{subfigure}[Sample1]{0.5\linewidth}
    	\includegraphics[height=40mm]{fig/conv1.jpeg} 
        \caption{An example of a convolutional layer.}
        \label{fig:conv_layer}   
    \end{subfigure}
    \qquad
    \begin{subfigure}[Sample1]{0.5\linewidth} 
    	\includegraphics[height=40mm]{fig/conv2.jpeg}
        \caption{Model of a neuron.}   
        \label{fig:neuron}
    \end{subfigure} 
    \caption{Examples of neural networks concepts from \cite{karpathy2016cs231n}.}
\end{figure}

The convolutional layer is a set of neurons placed in a grid of size $m \times n \times k$. Each neuron is connected to a local region in the previous layer. Each neuron performs the function $f(\bold{\omega} \cdot \bold{x} + b)$ shown in the figure \ref{fig:neuron}. Thanks to their regular structure and sharing parameters they perform a convolution function, where they look for different features in the previous layer. The neurons in lower layers can detect edges or lines, while neurons in higher layers can detect eyes or wheels. The neurons can be in a sparser grid relative to the previous layer with gaps called stride, downsampling the previous layer. 

A better insight into this phenomena was described in \cite{zeiler2014visualizing}.

\subsubsection{Pooling layer}
\begin{figure}[h!]
    \begin{subfigure}[Sample1]{0.5\linewidth}
        \includegraphics[height=40mm]{fig/pool1.jpeg} 
        \caption{Pooling downsamples the previous layer.}
        \label{fig:conv_layer}
    \end{subfigure}
    \begin{subfigure}[Sample1]{0.5\linewidth} 
        \includegraphics[height=40mm]{fig/pool2.jpeg}
        \caption{Max pooling operation.}   
        \label{fig:neuron}
    \end{subfigure} 
    \caption{Examples of pooling concepts from \cite{karpathy2016cs231n}.}
\end{figure}

The image has usually high resolution, but the information gained can be described in the much smaller information. Because at the lower levels we care about the relative positions of different features, such as lines or colors a lot, with the higher level, where the generalization is bigger, the dimension of the network usually becomes smaller. Pooling neuron takes a set of inputs and returns only one output as the highest input(max pool) or the average of the inputs(average pool). The pooling layer\cite{scherer2010evaluation} is an important part of almost all convolutional neural networks and it can not be trained since it does not have any parameters.

\subsubsection{Fully connected layer}
In the fully connected layer, each of the neurons is connected to every neuron in the previous layer. Since the layers can have many neurons, this is a very expensive layer and is usually performed in the last layers of the network, where the layers are smaller. 

\subsubsection{Overfitting and dropout layer}
Neural networks have many parameters and high Vapnik-Chervonenkis dimension \cite{blumer1989learnability}, therefore it is more prone to overfitting on small datasets. The network should figure out from the training set of examples some basic understanding of the problem and use this knowledge to process a sample, that it has never seen before. Overfitting means, that the network performs well on the training set by learning it by heart, but fails on the test set. This is a general phenomenon in machine learning. Usually, the way is to select a simpler classifier, that can still handle the problem, regularization(penalizing high weights) or increase the training data. Neural networks have come up with a solution called dropout, which does not solve the problem completely, but greatly helps.

A neuron in a dropout layer has only one input and during training randomly copies its value to the output or returns a zero. This forces the network to build more robust connections. This layer is active only during training. 


\subsection{Backpropagation}
\label{sec:back-prop}
As mentioned before, the learning of artificial neural networks is very different from learning of a real brain. With each input example, the network performs a forward pass. The information flows from one layer to next layer and the operations are performed until the information reaches the output layer. Information is compared with the ground truth embedding and a backpropagation is performed, that changes the parameters of the network in a way, that next time the output is closer to the required embedding. The difference is called loss and there are different ways to compute it.

The simplest square loss is computed as 

\begin{equation}
 L(\bold{y}) = - \sum_{i=1}^N(y_i -  \hat{y_i})^2.
\end{equation} 

After each forward pass, backpropagation finds a gradient for each weight and bias $\omega$, that minimizes the loss and changes it in the direction. 

\begin{equation}
\omega_{t + 1} = \omega_{t} + \frac{\partial L(\bold{y_t})}{\partial \omega}
\end{equation}

This general process is called a gradient descent. Backpropagation is an efficient way how to compute these partial derivatives from back to the front of the network based on the chain rule.

\subsection{Transfer learning}

Since filters in lower layers detect just very simple features, in comparison to higher layers, they are not too domain specific. If a network was trained on images of animals and we wanted to retrain it to detect vehicles, the higher level filters for detecting eyes or legs could not be used. In contrast, the low-level features detecting edges or lines could stay unchanged. This is the high-level idea of transfer learning. Training the network on a different domain and retraining it for a different one. 

This has many advantages. Training of the lower layers is more difficult because of the vanishing gradient problem \cite{hochreiter1998vanishing}. Training of these layers also requires a huge amount of data and computational time. There are many different trained networks available online. Using such a network and only fine tuning it \cite{hinton2006reducing} to a specific task requires also less training data. However, this is possible only for the exact same architectures of the networks.

\subsection{Frameworks}
Artificial neural networks require sometimes billions of very simple operations. Although they have been known for many years\cite{widrow199030}, only recently they have had a great success. That is due to increased computational power, parallelizing the computations on GPU, which perform these operations much faster than CPU and access to a big amount of data. 

There are also some frameworks, that offer very fast computations on GPU such as Google's TensorFlow \cite{abadi2016tensorflow}, Theano \cite{bergstra2011theano} or CNTK\cite{seide2016cntk} by Microsoft, from which the TensorFlow is the most common and the reidentification part of the thesis described in the section \ref{sec:similarity} was implemented in this framework. 

There are also some libraries such as Keras \cite{chollet2015keras} and Caffe \cite{jia2014caffe} that make implementing deep learning in C++ or python much easier. The SSD object detection network described in the section \ref{sec:ssd-implementation} was implemented in Keras.


\clearpage
\section{Classification, Detection and Reidentification networks}

The Neural networks are the state of the art for all kinds of image processing. The easier task is image classification, for which the driving force has been the ILSVRC \cite{deng2009imagenet} competition. This task is considered to be solved, but networks for classification are being used as a backbone (base network) for object detection networks. VGG \cite{simonyan2014very} described in the section \ref{sec:vgg} was used in this thesis as a base network for SSD \cite{li2017fssd} described in the section \ref{sec:ssd}. The Google Inception \cite{szegedy2015going} described in the section \ref{sec:inception} was used as a base network for the Facenet \cite{facenet} described in the section \ref{sec:facenet}.

\subsubsection{VGG}
\label{sec:vgg}

\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{fig/vgg.png}
\caption{The different VGG architectures. The ReLU function is not shown for simplicity. \cite{simonyan2014very}}
\label{fig:vgg}
\end{figure}

The VGG\cite{simonyan2014very} architecture from Oxford is one of the simplest ones but is very accurate. It is often used as a base network in object detection networks such as YOLO\cite{redmon2016you}, ARTOS\cite{barz2017fast}, SSD \cite{liu2016ssd} or Faster R-CNN \cite{faster-rcnn}. In this thesis, the VGG network was used as the SSD backbone and was adjusted for this specific domain in the section \ref{sec:ssd-implementation}. 

The VGG is an image classification network. It takes an input image and produces probabilities for each predefined class. It was inspired by \cite{ciresan2011flexible} and \cite{krizhevsky2012imagenet}.

The input resolution of the network is fixed to 224 $\times$ 224. The idea of the network is to use very small filters 3 $\times$ 3  (which is the smallest size to capture the notion of left/right, up/down, center)\cite{simonyan2014very}. Two of these filters stacked on top of each other create a receptive field of 5 $\times$ 5 and three have the receptive field of 7 $\times$ 7. With sharing parameters, the three layers have almost half the parameters, then the layer with 7 $\times$ 7 filters. This is different from \cite{krizhevsky2012imagenet} with 11 $\times$ 11 filter in the first layer and \cite{zeiler2014visualizing} with 7 $\times$ 7.

To bring more non-linearity, the VGG16 uses filters with 1 $\times$ 1 filters. This was used also in \cite{lecun1989backpropagation} as a network in a network.

Thanks to padding and the stride 1 of convolutional layers, the layers are down-sampled only by 2 $\times$ 2 max-pool layers with stride 2. The number of channels increases for better processing of higher level features. 

Different VGG architectures were designed in \cite{simonyan2014very} and their architectures are shown in the figure \ref{fig:vgg}. 

\subsection{Inception}
\label{sec:inception}

\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{fig/inception.png}
\caption{Inception module \cite{szegedy2015going} with dimension reductions.}
\label{fig:inception-module}
\end{figure}

The Google Inception network \cite{szegedy2015going} was used as the base network for Facenet \cite{schroff2015facenet} described in the section \ref{sec:facenet}, that was used for computing vehicle similarity. Inception was the winner of the 2014 ILSVRC \cite{deng2009imagenet} with the 6.67\% top-5 error rate. It has only 4 million parameters compared to AlexNet \cite{krizhevsky2012imagenet} with 60 million and VGG16 \cite{simonyan2014very} with 138 million parameters. A network with a low number of parameters is easier to deploy on less powerful machines and is less prone to overfitting.

They achieved this by introducing an inception module shown in the figure \ref{fig:inception-module}. These are basically small models inside a bigger model. The idea is, that when adding a layer, one must decide which layer to use. The Inception module combines all these possibilities together and performs a $1 \times 1$ convolution, $3 \times 3$ convolution, $5 \times 5$ convolution and $3 \times 3$ pooling layer.

Apart from classification training, Inception network was used for face identification \cite{facenet} and object detection \cite{ning2017inception, szegedy2014scalable}.

\subsection{SSD network for detection}
\label{sec:ssd}

The state of the art in detection is at the time the Single Shot MultiBox detector \cite{liu2016ssd}. This network provides accurate real-time object detections (74.3\% mAP at 59fps on VOC2007 on GPU). They achieve much faster speed, compared to previous Faster R-CNN (7 fps on GPU) thanks to eliminating the bounding box proposals. Since the SSD architecture is much simpler and compact than R-CNN architectures, the training is performed end to end on a single model, instead of training multiple networks for region proposal, classification and bounding box regression.

\subsubsection{Architecture}

\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{fig/SSDvsYOLO.png}
\caption{Comparison of the SSD\cite{liu2016ssd}(300x300) and YOLO\cite{redmon2016you}(448x448) architectures.}
\label{fig:ssd_vs_yolo}
\end{figure}

The SSD network uses one feed forward flow to produce a fixed number of bounding boxes and their probability for each class. The early layers are sometimes called a base network or a backbone. They are a classical architecture for image classification. 

Some additional layers were added on top of the base network shown in fig. \ref{fig:ssd_vs_yolo}. Multi-scale feature maps for detection are convolutional layers that progressively decrease their size and allow cheap detections of multiple scales. Convolutional predictors for detection can from feature maps of size $m \times n$ with $p$ channels can predict scores for categories or the offsets for each $3 \times 3$ element  using small kernels of $3 \times 3 \times p$. With decreasing feature maps, the predictions relate to different object scales. The bounding box offsets are measured relative to each feature map location. 

\subsubsection{Default boxes and aspect ratios}
Each feature map cell, from the feature maps at the end of the network, is associated with a set of bounding boxes. They differ in shape and scale as shown in the figure \ref{fig:ssd8}. Since the feature maps are computed from convolutions and max pool layers, the bounding boxes regularly tile the input image as shown in the figure \ref{fig:ssd8} with fixed positions.

A prediction of a probability of each class and predicted offset of the bounding box is computed. The offsets are given relative to the associated fixed position of the bounding box as shown in the figure \ref{fig:ssd4}. For predicting $k$ shapes and $c$ classes with 4 numbers representing the translations $\Delta(cx, cy, w, h)$, we need $k(c+4)$ filters for each feature map cell and for one $m \times n$ feature map layer we need $mnk(c+4)$ filters. The default boxes are similar to anchor boxes in Faster R-CNN with the difference, that SSD uses multiple feature maps for different object scales. 


\begin{figure}
    \begin{subfigure}[Sample1]{0.3\linewidth}
        \includegraphics[height=35mm, width=35mm]{fig/ssd_dog_cat.png} 
        \caption{SSD prediction.}
        \label{fig:ssd_detection}   
    \end{subfigure}
    \quad
    \begin{subfigure}[Sample1]{0.3\linewidth} 
        \includegraphics[height=35mm, width=35mm]{fig/ssd_feature_map.png}
        \caption{8 $\times$ 8 feature map.}   
        \label{fig:ssd4}
    \end{subfigure} 
    \quad
    \begin{subfigure}[Sample1]{0.3\linewidth} 
        \includegraphics[height=35mm, width=35mm]{fig/ssd_4_feature_map.png}
        \caption{The predicted offsets.}   
        \label{fig:ssd8}
    \end{subfigure} 
    \caption{The SSD \cite{liu2016ssd} predictions.}
\end{figure}

\subsection{Loss}
The network is trained with two training objectives. The first one is the classification loss $L_{conf}$ and the second one is the localization $L_{loc}$ loss. These two losses are added to a unified loss $L$ and propagate together. The loss according to \cite{li2017fssd} is:

\begin{equation}
L(x, c, l g) = \frac{1}{N}L_{conf}(x, c) + \alpha L_{loc}(x, l, g)
\end{equation}

where $N$ is the number of matched boxes, $x = \{0, 1\}$ is indicator for matching default boxes to ground truth boxes, $c$ is the confidence of each predicted class, $l$ is the predicted bounding box and $g$ is the ground truth box and $\alpha$ is a weighting constant. The $L_{conf}$ is the cross-entropy maximizing the ground truth class confidence and minimizing other classes. $L_{loc}$ maximizes the Jaccard overlap of good predictions using the Smooth L1 loss \cite{girshick2015fast}. Both of these losses are easily differentiable and can be backpropagated. 
 

\subsubsection{Training}
With each training image, the ground truth boxes need to be assigned to a default bounding box. The ground truth box is matched to all default boxes, which have the Jaccard overlap higher than 0.5. This is more robust than picking only the one box with the highest overlap as in MultiBox \cite{erhan2014scalable}. After that, the training is performed end to end by a standard backpropagation. 

The training phase also includes a positive and a hard negative mining. Not all predicted boxes participate in training. Most of the predicted boxes are usually negatives. The backpropagation is on all the boxes matched with ground truth box as well as the negatives with the lowest score for the background. This is called hard negative mining and it speeds up the convergence. The negatives are picked with the ratio 3:1 to the positives.

\subsection{Non-maxima suppression}

\begin{figure}[h!]
	\begin{subfigure}[Sample1]{0.5\linewidth}
    	\includegraphics[height=70mm, width=70mm]{fig/nms1.png} 
        \caption{Predictions before applying nms.}
        \label{fig:nms1}   
    \end{subfigure}
    \quad
    \begin{subfigure}[Sample1]{0.5\linewidth} 
    	\includegraphics[height=70mm, width=70mm]{fig/nms2.png}
        \caption{Predictions after applying nms.}   
        \label{fig:nms2}
    \end{subfigure} 
\caption{Non maxima suppression\cite{diez2011non} keeps a single prediction.}
\label{fig:nms}
\end{figure}

Detectors usually create many predictions around an object as shown in the figure \ref{fig:nms1}. The goal of non-maxima suppression is to keep only a single bounding box. 

Each prediction has a confidence. Non-maxima suppression sorts all predictions by their confidence. It takes a prediction with the higher confidence and finds all predictions with a Jaccard overlap\cite{tan2005introduction} higher than some constant, removes them and moves to next prediction. This is a naive approach and use of k-d trees can speed up the process.

The constant is the only parameter of the algorithm and setting it is a very important thing. If set too high, objects will have multiple detections. If set too small, objects, that are close together, will be detected as one. The rule of thumb is to set the constant between 0.3 and 0.5.


\subsection{Facenet for reidentification}
\label{sec:facenet}
Google Facenet \cite{schroff2015facenet} is a deep neural network for computing similarities between faces but can be retrained for computing similarities between objects from any domain, such as vehicles. This network receives an input image in RGB with the size of $220 \times 220$ and returns an embedding vector with the length of 128. The Euclidean distance between samples directly corresponds to the similarity of objects. Faces of the same person are mapped to the similar place in the Euclidean space. Once we have these metrics, recognition becomes a K-NN and verification is a simple thresholding of the distance. The goal is, that the embeddings will be invariant to pose, illumination and other factors. For computing, the similarity of an image with more images representing the same identity, a mean of the distances is taken.

\subsubsection{Architecture}
\begin{figure}
    \begin{subfigure}[Sample1]{0.5\linewidth}
        \includegraphics[width=0.95\linewidth]{fig/facenet.png} 
        \caption{The model structure.}
        \label{fig:facenet_structure}
    \end{subfigure}
    \quad
    \begin{subfigure}[Sample1]{0.5\linewidth} 
        \includegraphics[width=0.95\linewidth]{fig/triplet_loss.png}
        \caption{The learning objective.}   
        \label{fig:triplet_loss}
    \end{subfigure}
    \caption{The Facenet \cite{schroff2015facenet}.}
\end{figure}

The facenet architecture is mostly an image classification network for 128 categories and a normalization layer. Google has tried various architectures, such as Zeiler\&Fergus \cite{zeiler2014visualizing} (with the input resolution 220 $\times$ 220), Google Inception \cite{szegedy2015going} (224 $\times$ 224, 160 $\times$ 160, 96 $\times$ 96) and mini Inception (165 $\times$ 165) and tiny Inception(140 $\times$ 140). 


The \cite{szegedy2015going} shows, that the embedding space dimensionality of 128 is enough for recognizing faces.

\subsubsection{Training}
The training and evaluation data are sets of images of different people. The goal of the network is to project images of the same person close to each other and different people far from each other as shown in the figure \ref{fig:triplet_loss}. This is a different problem, the classification, where a finite number of categories is previously known. There have been attempts \cite{wst2008deeply, taigman2014deepface}  at training network for classification and then removing the last classification layers, but with less success.

The introduced approach is different from an image classification training and is called triplet loss. With each training step, three examples are selected. Two of the same class and one from a different one. One of the positives is called anchor, the one from the same class is called a positive and the third one a negative sample. The goal is to change the network parameters in a way, that the positive embedding is moved closer to the anchor and the negative further from the anchor.

The triplet loss is represented as $f(x) \in R^d$, where $x$ is the input image that is transformed into an Euclidean $d$-dimensional space. The last normalization layer normalizes the embedding to a hypersphere of the size 1, meaning $\|f(x)\|_2 = 1$ 

To ensure, that the model is consistent, meaning, that there exists a threshold, that will correctly classify every pair of the images, we need to ensure, that for $\alpha = 0$

\begin{equation}
\begin{aligned}
&\|f(x_i^a) - f(x_i^p)\|^2_2 + \alpha < \|f(x_i^a) - f(x_i^n)\|^2_2 \\
&\forall (f(x_i^a), f(x_i^p), f(x_i^n)) \in T
\end{aligned}
\end{equation}

The $\alpha$ is a constant, that can enforce a margin. $T$ is a set of all possible combinations of images in the training set.

The loss, that is being used for training according to \cite{szegedy2015going} is 

\begin{equation}
\sum_i^N [\|f(x_i^a) - f(x_i^p)\|^2_2 - \|f(x_i^a) - f(x_i^n)\|^2_2 + \alpha]_+.
\end{equation}

After a couple of steps of training, most of the images will be classified correctly and the constraint will be met. For fast convergence, it is crucial to select those triplets, that violate the condition. This is called hard positive and hard negative mining.

There were recently some improvements of triplet loss in form of quadruplet loss \cite{chen2017beyond}, that enforces greater margins among classes.

\clearpage
\subsection{Multi camera tracking}
\label{sec:multi-camera-tracking}
After solving the task of detection, single camera tracking, and similarity metrics, The task needs to be extended to the whole city. It is important to note, that the city with built cameras in lamps does not exist at the time of writing the thesis, therefore these ideas cannot be tested on the whole city. Only a theoretical approach was explored. It is also important to keep in mind the task of tracking one vehicle in a city, therefore not tracking all vehicles at once, which can 


\clearpage
\section{Implementation}

The deep learning runs in TensorFlow \cite{abadi2016tensorflow} framework. Keras \cite{chollet2015keras} was used for implementation and development of neural networks, opencv \cite{opencv} for video and image operations and numpy \cite{walt2011numpy} for fast vector operations. The whole project was written in python.


\subsection{Mask R-CNN segmentation}
\label{sec:mask-rcnn}

\begin{figure}[h!]
    \begin{subfigure}[Sample1]{0.47\linewidth}
        \includegraphics[width=0.95\linewidth]{fig/Mask-RCNN-road.png} 
        \caption{Mask R-CNN on a standard street camera.}
        \label{fig:mask-street-cam}
    \end{subfigure}
    \quad
    \begin{subfigure}[Sample1]{0.47\linewidth} 
        \includegraphics[width=0.95\linewidth]{fig/Mask-RCNN-omni.png}
        \caption{Mask R-CNN on a fisheye camera has problems recognizing vehicles from the top.}   
        \label{fig:mask-omni-cam}
    \end{subfigure}
    \caption{Examples of Mask R-CNN network \cite{he2017mask}.}
    \label{fig:facenet}
\end{figure}

First, a couple of approaches were explored. One of them is Mask R-CNN\cite{he2017mask}. This network builds on detections and provides segmentation. This gives more information since we know exactly what parts of objects belong to which entity, making the tracking simpler. As shown in the figure \ref{fig:mask-street-cam}, Mask R-CNN can very well detect standard traffic scenes, however, in the figure \ref{fig:mask-omni-cam}  one can see, that it has problems detecting vehicles from the top. This is due to a very small number of vehicles viewed right from the top in the Microsoft COCO dataset \cite{lin2014microsoft}. 

Since there are no datasets for training and evaluating segmentation from the top-places fisheye street cameras the performance was not measured. Transfer learning could be applied, but creating such a segmentation dataset is very expensive to create. Combining this fact with the 5 fps of this network, it is not feasible for real-time application and this approach was not explored further. 

However, it is a valuable insight and when segmentation becomes faster, this is definitely an approach worth looking into in the future work.

\subsection{SSD detector}
\label{sec:ssd-implementation}
The most important part of the thesis is the vehicle detection. This is a very hard problem given the conditions. First of all, since the camera is fisheye, it is not possible to simply use an off the shelf network trained for example on ImageNet or on PASCAL VOC and use it right away. 

For the detector was chosen the SSD network \cite{liu2016ssd} described in the section \ref{sec:ssd}. It is the state of the art real-time detector outperforming the R-CNN\cite{faster-rcnn} and YOLO\cite{redmon2016you} in speed and accuracy as described in the section \ref{sec:object_detection}. 

The SSD used VGG16 for the base network, which is described in the section \ref{sec:vgg}. VGG networks are popular among detection networks for their performance and simplicity. 

The SSD introduces two versions differing in the input resolution: SSD300 and SSD512 with the resolutions $300 \times 300$ and $512 \times 512$ respectively. Because the cameras are positioned high and because of the fisheye view, objects, especially bicycles and motorcycles, can appear very small. It is sometimes a problem for humans to detect them on $1024 \times 1024$ image, therefore the version with higher resolution was chosen. 

A working network implemented in keras\cite{ssd_keras} was used as a starting point, but some modifications were made. 

\subsubsection{Temporal difference}

\begin{figure}
    \begin{subfigure}[Sample1]{0.45\linewidth}
        \includegraphics[width=1\linewidth, height=70mm]{fig/stream_frame.png} 
        \caption{A frame from the street camera.\\ \quad}
        \label{fig:td_frame}
    \end{subfigure}
    \quad
    \begin{subfigure}[Sample1]{0.45\linewidth} 
        \fbox{\includegraphics[width=1\linewidth, height=70mm]{fig/stream_black.png}}
        \caption{Temporal difference highlights moving objects.}
        \label{fig:td_black}
    \end{subfigure}
    \caption{Temporal difference helps to detect moving objects.}
    \label{fig:td}
\end{figure}

As described in the section \ref{sec:object_detection}, the standard pipeline for video detection is detecting each frame independently. This loses much information, that can be gained from the video. Cameras used in this thesis are static and the vehicles, that are to be detected are mostly moving. A cheap segmentation of the scene is a temporal difference. 

A difference between two subsequent frames is shown in the figure \ref{fig:td_black}. It is computed as:

\begin{equation}
D(F^{T}_{i, j}) = max(255, \sum_{c = 1}^3 |F^t_{i, j, c} - F^{t-1}_{i, j, c}|),
\end{equation}

where $F_{i, j}^t$ is a value of a frame at the time $t$, $i$-th row and $j$-th column. The image is represented in 8-bit unsigned int, therefore to prevent overflow, the value is saturated on 255. The difference between frames can be computed very fast in python using numpy\cite{walt2011numpy} and opencv\cite{opencv} libraries. 

There are more ways, how to use this information in the network. One could feed just this layer and probably achieve good detections but would have problems with object classification by losing the RGB information. The introduced approach keeps both the information by feeding both to the network.

The SSD network was extended for an extra input channel in addition to RGB, that is the temporal difference. The data have size $H \times W \times 4$ instead of $H \times W \times 3$ for standard images. This version of the network was temporarily called RGBD network. The D stands for the difference and should not be confused with commonly used distance.

In some cases, parked cars should not be detected. This is in a case of a road with a lot of parked cars and where are no traffic lights. In this case, the tracking will perform better with less bounding boxes and no occlusions with the parked cars. This can be easily done by computing the average value in the difference channel and thresholding it.


\clearpage
\subsubsection{Architecture}
The architecture is similar to the original SSD512 architecture, but some changes were made. The input layer was extended to the differential channel as described before. 

According to \cite{cao2018feature}, SSD might have a problem with detecting small objects. Therefore one more feature layers were added with a small scaling factor of 0.05, compared to the original 0.1. This additional feature layer is able to detect roughly twice as smaller objects as the original network, such as pedestrians, bicycles or motorcycles.

\subsubsection{Dataset}
\label{sec:ssd-dataset}
A scene annotating was introduced in the section \ref{sec:data-generation}. However, this approach could not be used for SSD training and was used only for similarity training described in the section \ref{sec:similarity}. Even though it produces no false positives, sometimes it produces false negatives due to the inability of correctly segmenting overlapping objects. It only detects such a situation and does not label anything.

Despite over-all good scene annotation, the SSD's hard negative mining strategy selects the bounding, that the network predicted most likely as a presence of an object and was not labeled and tweaks the weights in such a way, that next time it is predicted as a background. In other words, if a vehicle is not labeled in the image, the network might detect it but will be corrected, that it is a background. This would lead to a bad learning, overfitting and might not even converge.

A standard online annotation tool was provided by the Good Vision company, that allows annotators to draw bounding boxes and attach classes to images. Several annotators have annotated 1764 images.

The Brazilian party provided 15 videos from different scenes, each about 1 hour long. Each video was shot from a slope on a car, as shown in the figure \ref{fig:calibration}. Only two cameras in lamps were working and recording video at the time of writing this thesis since the whole project is still in development. 

The RGB images and the temporal difference was extracted from the total of 17 scenes a second apart and the images without traffic were deleted. There is a need for many scenes for the diversity of the roads, surroundings and lighting conditions. A detector can easily overfit on a scene by learning its background, therefore the test data were chosen to be one scene, where the detector did not learn.

The dataset contains 1654 training and 110 testing images. Because of the sparsity of the data and not many hyperparameters, the validation set is equal to the test set. There are 7 classes in the data: [person, bicycle, car, bus, motorbike, truck, animal]. The classes person and animal are not important for this project but might be used for adding new features in the future.
\subsubsection{Data augmentation}
For extending the dataset, many different methods of editing the images are being used. They usually contain horizontal flip, selecting patches or editing brightness. 

The nature of the 360-degree camera enables us to rotate the image in an arbitrary angle, not only a horizontal flip, such as on standard datasets. 

On a standard frame, not all vehicles are moving. Some are parked and some stay at a traffic or on a red light. It is important to detect these also, therefore 4th layer is set to zero with the probability of 0.1 to provide more data for the RGB part of the network.

Instead of creating a new augmented dataset, augmentation adjustments were applied on each incoming sample during training. All the data augmentation techniques used are:

%\vspace{-40mm}
\begin{itemize}[itemsep=5pt,parsep=2pt]
\item Random rotation of the image.
\item Random brightness change.
\item Random image translation.
\item Random scaling of the image.
\item Random setting of the 4th channel to zero.
\end{itemize}

\subsubsection{Training}
It is not possible to use a pre-trained model on ImageNet\cite{deng2009imagenet} or COCO \cite{lin2014microsoft}, because of the changed architecture of the network. A pre-trained weights could not be used for transfer learning. An attempt was made to pre-train the model on Youtube-bounding boxes dataset\cite{real2017youtube}, but only static videos would have to be selected because of the differential input channel and the scenes are too different from the fisheye traffic camera.

The images were shuffled before each epoch. The minimum overlap between the anchor and a label was set to 0.5, the maximum overlap between anchor and background label was set to 0.2 and the threshold for moving labels from the ground truth during training was set to 0.2. The training was performed on the NVIDIA GeForce GTX 1080 for 4 days.

A custom tensorboard callback and mAP computation had to be implemented for observing the training progress and evaluating the model. Other features were added, such as tracking the detection ratio or computing the best threshold from the mAP curve. 

The Adam optimizer \cite{kingma2014adam} was used with parameters: The learning rate $10^{-3}$, $\beta_1 = 0.9, \beta_2 = 0.999, \epsilon=10^{-8}, decay = 0.0005$. These parameters were selected by \cite{ssd_keras} and were not changed. The training process is shown in the figure \ref{fig:ssd_training}

The training converged after estimated 3000 training steps. The final training loss was 0.3035 ant validation loss 0.2201. The mAP on the test set was 91,6 \%.


\begin{figure}[H]
    \begin{subfigure}[Sample1]{1\linewidth} 
    	\includegraphics[width=0.95\linewidth]{fig/ssd_rgbd/mAP.png}
        \caption{The mean average precision on validation set.}   
        \label{fig:ssd_mAP}
    \end{subfigure}
    \begin{subfigure}[Sample1]{1\linewidth} 
    	\includegraphics[width=0.95\linewidth]{fig/ssd_rgbd/loss.png}
        \caption{The loss on the training set.}   
        \label{fig:ssd_loss}
    \end{subfigure}    
	\begin{subfigure}[Sample1]{1\linewidth} 
    	\includegraphics[width=0.95\linewidth]{fig/ssd_rgbd/val_loss.png}
        \caption{The loss on the validation set.}   
        \label{fig:ssd_val_loss}
    \end{subfigure}    
    \caption{The process of training the 4 channel input SSD network. The training was performed on the NVIDIA GeForce GTX 1080 for 4 days.}
    \label{fig:ssd_training}
\end{figure}




\subsection{Single camera tracking}

Single camera tracking is the only thing, that was not implemented by the author of the thesis. The company Good Vision provided their own state of the art tracker. It is similar to the tracker explained in the section \ref{sec:optical-flow}, but more advanced.

It requires the video and a sequence of detections and connects these detections to tracks, which are sequences of these detections. This allows non-supervised clustering of track in the scene without needing to mark the initial object. 

The Good Vision tracker works in three steps called seeding, displacement, and matching. 

\subsubsection{Seeding}
When a new detection appears, a set of features is initialized. These are a set of points inside the bounding box that were selected by the Shi-Tomasi \cite{shi-tomasi} feature detector. The Shi-Tomasi chooses features by finding local maxima in differentiating the image in both axes and choosing the smaller of the two. It usually finds corners or other distinguishing features, that are easier to track by optical flow \cite{optical-flow}. 

Because the detection is a bounding box, the features are initialized not only on the tracked vehicle but also on the background. This is undesirable since we want to track only the moving object. This could be solved by replacing object detection with segmentation but would increase the computational time of the detector and make creating datasets harder.

\subsubsection{Displacement}
After features were initialized, displacement of each feature is computed with the incoming frame. The movement is estimated by optical flow using Lukas Kanede \cite{lucas-kanede} method. Optical flow is computed only for the sparse local features and not for the whole frame, making it faster. Those features, that were initialized on moving vehicle move with it and those on the background stay in the same place. 

\subsubsection{Matching}
Each feature remembers, which object it originally belonged. After the features were moved in the displacement step, they are matched to the new detection. The features outside detections(those previously seeded on the background) are removed. The detections with features, that belonged to the same previous detection, take its identity and are matched together. This way a track is created.

\subsection{Similarity}
\label{sec:similarity}

\begin{figure}
    \begin{subfigure}[Sample1]{0.5\linewidth}
    	\includegraphics[width=0.95\linewidth]{fig/facenet_eval_table.png} 
        \caption{Mean VAL at $10^{-3}$ FAR.}
        \label{fig:facenet_table}
    \end{subfigure}
    \quad
    \begin{subfigure}[Sample1]{0.5\linewidth} 
    	\includegraphics[width=0.95\linewidth]{fig/facenet_eval.png}
        \caption{Comparison of different DNN architectures.}   
        \label{fig:triplet_loss}
    \end{subfigure}
    \caption{Comparison of various facenet\cite{szegedy2015going} base networks evaluated on Labeled faces in the wild\cite{huang2007labeled} and YouTube faces\cite{wolf2011face} datasets.}
    \label{fig:facenet}
\end{figure}

The neural network Facenet \cite{schroff2015facenet} was chosen as the metrics of similarity among vehicles. As described in the section \ref{sec:facenet}, it is designed to compute the similarity between faces and is taught to be invariant to various poses and illumination conditions. The network can be retrained for a different domain apart from faces, in our case vehicles. Given an image of a car, the network will produce an embedding in a 128-d Euclidean space in such a way, that the same objects should be close to each other. Google has tried many architectures, that are compared in the figure \ref{fig:facenet}. The Inception network NN3 with the input resolution of 160 $\times$ 160 is a good trade-off between the speed and accuracy. Furthermore increasing input resolution is not practical for our task, since the resolution of the cameras is only full HD and the vehicles inhabit only very small area in the frame.

As a starting point, TensorFlow implementation of the network \cite{facenet} was used. The network stayed unchanged, but the data handling, tensorboard callbacks, and evaluation had to be implemented.

\subsubsection{Dataset}
\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{fig/facenet/labeling.png}
\caption{An example of a one object in a training set.}
\label{fig:facenet-labeling}
\end{figure}

The dataset needed for this task is in a form of a set of images of an object for many objects. That means, that we need set of images of the same vehicle for many vehicles. Use of other datasets, such as \cite{standford} is not possible, because their images look too different, from those in the fisheye street camera.

The main idea in creating a custom dataset was, that if we have a track - a set of bounding boxes of the same car on one scene, we can use these detections as a set of images of the same vehicle. That is exactly what the background subtraction and optical flow do. This dataset was created by a semi-supervised annotation tool based on background subtraction and optical flow described in the section \ref{sec:data-generation}.

Different scenes and different weather conditions were used for better diversity. The created dataset contained over 9 000 images.

\subsubsection{Problems with the dataset}
When the facenet network was trained on this dataset, it did not work at all. There are a couple of big problems that have to do with the way Facenet learns. In each training step, it takes two images, that belong to different classes, but were classified as too similar. If each class contained a different car, this would make no problem. However, vehicles, especially in South America, are very similar. If there is a model of the same car in the database more times, the facenet will be taught, that it is a different car and it needs to change. Especially if the car is in the same scene, it looks almost identical and not even human can tell them apart. 

Another problem is, that if the same model of the car appears on a different scene, there is no match with the original car. The network treats these cars as different. This is the opposite of what we want to achieve.

The final problem considers low diversity among one track. The road is usually of the same color and the lightning conditions barely change. The network can then overfit on these features and will not generalize.

\subsubsection{Improving the dataset}
These problems had to be taken into account when creating a new version of the dataset. It would be very hard for the annotator to remember, which types of vehicles have passed and which did not. Instead, the original dataset had to be clustered. That means putting same vehicles from different scenes into one class. This is a very time demanding process, but the results are worth it.

\subsubsection{Training}
\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{fig/facenet/acc.png}
\caption{Accuracy during the facenet training.}
\label{fig:facenet_training}
\end{figure}

To extend the dataset, some augmentation techniques were used, such as rotation of the vehicles by 90\textdegree. Transfer learning was used and the network pre-trained by Google on faces was fine-tuned on vehicles. The training took 12 hours (150 000 steps) on the NVIDIA GeForce GTX 1080. The training set contained 8800 images and the validation set 884 images from the clustered dataset. These were models of a vehicle, that the network has not seen before. The accuracy was computed by changing the task to a classification task described in the section \ref{sec:similarity-eval}

\begin{figure}[H]
    \begin{subfigure}[Sample1]{0.5\linewidth} 
    	\includegraphics[width=0.95\linewidth]{fig/facenet/tpr.png}
        \caption{True positive rate.}   
        \label{fig:facenet}
    \end{subfigure}
    \begin{subfigure}[Sample1]{0.5\linewidth} 
    	\includegraphics[width=0.95\linewidth]{fig/facenet/tnr.png}
        \caption{True negative rate.}   
        \label{fig:facenet}
    \end{subfigure}    
	\begin{subfigure}[Sample1]{0.5\linewidth} 
    	\includegraphics[width=0.95\linewidth]{fig/facenet/fpr.png}
        \caption{False positive rate.}   
        \label{fig:facenet}
    \end{subfigure}
    \begin{subfigure}[Sample1]{0.5\linewidth} 
    	\includegraphics[width=0.95\linewidth]{fig/facenet/fnr.png}
        \caption{False negative rate.}   
        \label{fig:facenet}
    \end{subfigure}       
    \caption{Additional information about the Facenet training. The training was performed on the NVIDIA GeForce GTX 1080 for 12 hours.}
    \label{fig:facenet_training_additional}
\end{figure}

The final results are shown in the table \ref{tab:facenet}.

\subsubsection{T-SNE visualization}
\begin{figure}[h!]
\centering
\includegraphics[width=0.5\linewidth]{fig/tsne0.png}
\caption{Visualization of facenet embeddings using the T-SNE dimensionality reduction.}
\label{fig:facenet_training}
\end{figure}

The facenet transforms the images into a 128-dimensional Euclidean space. It is very hard to understand problems, such as the variance of rotation, lightning and so on. Simple distances between images are not enough to understand better of the embeddings. 

Some dimension reduction techniques can be used for the embedding visualization. The most common techniques are PCA\cite{wold1987principal} and T-SNE\cite{maaten2008visualizing}. PCA is better to understand the overall relations, while T-SNE can better visualize the local relations. T-SNE was chosen and a small batch of car images was selected. The T-SNE was computed in python using sklearn\cite{pedregosa2011scikit} package.

The T-SNE algorithm The results are shown in the figure \ref{fig:facenet_training}.

It can be seen, that it can distinguish different colors of cars. However being presented two very similar white vehicles, it has some problems distinguishing them. Also, we can see, that the network is not fully rotation invariant, even though it was trained to be. But we need to keep in mind, that a lot of information was lost with the dimensionality reduction, therefore the preview is not very reliable.


\subsection{City representation}
\label{sec:city-representation}
\begin{figure}[h!]
\centering
\includegraphics[width=1\linewidth]{fig/intersection.png}
\caption{An example of a part of city representation with transition probabilities.}
\label{fig:intersection}
\end{figure}

There is a couple of ways how to represent a city, its cameras, and vehicles. It does not make much sense to represent the city as a 3D model since the city is mostly 2D and an altitude of places would be complicated to precisely measure. 

If we had cameras in every lamp, this would be a different task, since we could use techniques for overlapping camera tracking. However, this is not the case. The cameras are only at some and the rest of the system is mostly unknown. 

Since vehicles drive only on roads, it would be a good idea to include this knowledge in the model, rather than not constraining the vehicles positions. The model should be also easily scalable. 

This leads to representing the city as a graph, rather than a 2D plane. All the cameras will be representing a set of vertices and connections between them are edges. Each vertex corresponds to a vehicle state. The edges don't necessarily represent roads directly since the cameras are not at every intersection. 

The city is represented by a Markov chain, where the probabilities of all the outgoing edges sum to 1.

When a vehicle is detected on one camera, the heading is an important piece of information and can be detected from the track. Therefore we create a different state for each vehicle direction as shown in the figure \ref{fig:intersection}. 

Each edge has a probability(transition function), that corresponds to the vehicle moving in that direction. Some probabilities are deterministic, for example, a long road with multiple cameras and no intersections. Some transitions are probabilistic, as shown in the figure \ref{fig:intersection} as $P_i$ or $1 - P_i$. When there is no camera at an intersection, only a probability can describe the vehicle's movement. In this case, each edge will have a constant probability assigned to it based on observation. This can be a function of for example daytime, but will not be changed otherwise. In case of a camera at the intersection, the probability will be adjusted based on the measurement of the camera, but still can deal with some uncertainty, especially when the camera is not sure, where the vehicle actually went. 

This probabilistic representation allows more advanced modeling of the city with some uncertainty.

Each of the vertices has some more parameters, which are the mean and variance of traveling between the vertices. This is obtained by observing the vehicles. This can also be a function of daytime or the traffic.


\subsubsection{Reidentification}
\label{sec:reidentification}
When expecting a vehicle in a camera, not only similarity but also a time and the estimated position of the target vehicle plays a role. An algorithm is presented, that takes all the information and combines it with a simple yes/no answer if a detected vehicle is the target one. If the target vehicle was at time $t_0$ at a state $s_0$ with the probability $p_0$ and there is a transition probability of $P_0^i$ to a state $s_i$ and the edge having a mean transition time $\mu_0^i$ with variance $\sigma_0^i$ and the detected similarity distance between the target vehicle and the detected one is $m$, then the score function is 

\begin{equation}
\label{eq:score-reid}
S(i, m, t) = m^{-1}p_0 P_0^i \frac{1}{\sigma_0^i \sqrt{2\pi}}e^{-\big{(}\frac{t_0 + \mu_0^i - t}{2\sigma_0^i}\big{)}^2}
\end{equation}

The score function of the $i$-th state with the measured similarity distance $m$ at time $t$ is thresholded and that gives us a robust yes or no answer if the detected vehicle is the target one. If more tracks are thresholded, the track with the most matches is considered to be the target vehicle.

This method describes a reidentification of one edge. If a vehicle is detected, the probability of the vehicle being at the time $t$ at state $i$ is 1, and the algorithm follows the vehicle on another edge. The score function expects a normal probability of appearance of the vehicle at another camera. 

The $\mu$ can also be a function of velocity and position of the car. For this, a precise calibration of the camera needed to be developed as described in the section \ref{sec:lens}

\clearpage
\subsubsection{Decreasing computational demands}
Because the detection is a computationally very expensive process and only one vehicle is being tracked, it does not make sense to run the detector on all cameras at once, especially in a big city for cameras being far away. The equation \ref{eq:score-reid} can be rewritten to

\begin{equation}
A(i, t) = \sum_{j \in T}p_0 P_0^i \frac{1}{\sigma_j^i \sqrt{2\pi}}e^{-\big{(}\frac{t_j + \mu_j^i - t}{2\sigma_0^i}\big{)}^2}
\end{equation}
where $T$ is the set of all the states $s_j$, that have an edge leading from $s_j$ to $s_i$. The score $A(i, t)$ is the probability of the target vehicle being at the camera. It can be thresholded and giving us the information about whether the camera can be turned off or not.
This is generally fast to compute.


\subsection{Multi camera tracking}

All the different parts of the thesis were put together to be able to detect a vehicle, detect it on another camera and recognize it based on the time of arrival, direction, similarity and the prior knowledge about the cameras and their relations. 

The tracking algorithm at one camera detects a track, from which many images of the target vehicle are extracted. This serves as a model for the reidentification task. 

Transition times and variances are computed prior to the program execution and remain constant. After a vehicle is detected, another camera starts the detection and tracking. Each detection is thresholded according to the equation \ref{eq:score-reid} and if one detection is classified as being the target identity, the whole track is declared to be the target vehicle and the model of the vehicle is extended.

\clearpage
\section{Evaluation}
Each of the modules was evaluated separately and the overall method was experimentally evaluated in a real-world scenario.

\subsection{Mean average precision.}
\label{sec:mAP}
Mean average precision (mAP) is the most used metrics for object detection problem. The advantage is, that it does not depend on the selected confidence threshold, but only on the IoU threshold. This metrics is not constrained only for object detection problems in vision but can be used for all detection problems. 

The algorithm for computing mAP runs for all thresholds. Given an arbitrary threshold, the predicted bounding boxes are those, whose confidence exceeds it. If there is a high IoU of a ground truth box and some predicted bounding boxes having the same class, the predicted box with the highest confidence is matched and considered true positive($TP$) and no other box can be matched with the ground truth bounding box. If a predicted bounding box is not matched with any ground truth bounding boxes, it is considered false positive($FP$). If a ground truth bounding box is not matched with any predicted bounding box, it is considered a false negative($FN$).

Precision($P$) corresponds to what portion of ground truth boxes were matched. With lowering the threshold, it can only increase, since more ground truth bounding boxes will be matched.
\begin{equation}
P = \frac{TP}{TP + FP}
\end{equation}

Recall($R$) corresponds to what portion of predicted bounding boxes were matched. 
\begin{equation}
R = \frac{TP}{TP + FN}
\end{equation}

The precision-recall curve in the Fig.\ref{fig:precision-recall} shows the dependency of precision and recall. The higher the precision, the lower the recall. The area below the curve is called average precision($AP$) and the mean overall classes are called mean average precision($mAP$).

\begin{equation}
mAP = \frac{1}{|C|} \sum_{c \in C} AP_c
\end{equation}

where $C$ is the set of classes.

\begin{figure}[H]
\centering
\includegraphics[width=1\linewidth]{fig/precision-recall.png}
\caption{Example of arbitrary precision-recall curve.}
\label{fig:precision-recall}
\end{figure}

\clearpage
\subsection{SSD object detection}
\label{sec:ssd-evaluation}
The implemented detector described in the section \ref{sec:ssd-implementation} was compared with the state of the art SSD \cite{liu2016ssd}. Because using pre-trained SSD on ImageNet would perform badly because of the domain specifications, the models were trained with the same parameters on the same dataset. 

\begin{figure}[H]
    \begin{subfigure}[Sample1]{1\linewidth} 
        \includegraphics[width=1\linewidth]{fig/ssd_compare/mAP.png}
        \caption{The mean average precision on validation set.}   
        \label{fig:ssd_mAP}
    \end{subfigure}
    \begin{subfigure}[Sample1]{1\linewidth} 
        \includegraphics[width=1\linewidth]{fig/ssd_compare/loss.png}
        \caption{The loss on the training set.}   
        \label{fig:ssd_loss}
    \end{subfigure}    
    \begin{subfigure}[Sample1]{1\linewidth} 
        \includegraphics[width=1\linewidth]{fig/ssd_compare/val_loss.png}
        \caption{The loss on the validation set.}   
        \label{fig:ssd_val_loss}
    \end{subfigure}    
    \caption{The process of training the introduced 4 channel SSD (orange) and the 3 channel SSD (blue) networks. The training was performed on the NVIDIA GeForce GTX 1080 for 1 day. The graph shows, that the presented solution performs much better than the state of the art SSD.}
    \label{fig:ssd_training}
\end{figure}

Because of the hardware limitations, the training of each network took one day. After deciding, that the difference layer greatly helps, the RGBD network was trained for 4 days. The final comparison is shown in the table \ref{tab:ssd_camparison}.

\begin{table}
\centering
\begin{tabular}{|l|c|c|c|c|}
  \hline
  models & Training time & Loss & Validation Loss & mAP \\
  \hline
  SSD512 (RGB) & 1 day & 1.127 & 1.055 & 63,2 \\
  \hline
  SSD512 (RGBD) & 1 day & 0.4583 & 0.3349 & 90,0 \\
  \hline
  SSD512 (RGBD) & 4 days & 0.3035 & 0.2201 & 91,6 \\
  \hline
\end{tabular}
\caption{Results on the custom dataset from the section \ref{sec:ssd-dataset} show, that the introduced RGBD architecture is more accurate, than the standard SSD.}
\label{tab:ssd_camparison}
\end{table}

The computational overhead given the 4th channel was tested, but the speed of both models are below recognition. That is mainly because the 4th channel is only in the input layer, and increases the number of parameters only in the first hidden layer, the other layers are unchanged.

\subsection{Facenet similarity}
The facenet was trained on NVIDIA GeForce GTX 1080 for 12 hours on 8800 training images and was tested on different 884 images. 

\subsubsection{Evaluation metrics}
\label{sec:similarity-eval}
Evaluating the similarity task was transformed to an evaluation a classification task. The task is to classify if a pair is positive (belong to the same class), or negative(does not belong to the same class). Given two images, compute the distance of their embeddings and compare it to a threshold. 

The threshold is found by cross-validation on the training set. The validation pairs are selected randomly in the ratio 1:1 of being positive and negative.

\subsubsection{Results}

The final results are shown in the table \ref{tab:facenet}


\begin{table}[H]
\centering
\begin{tabular}{|l|l|}
  \hline
  Accuracy & 0.8105 \\
  \hline
  True positive rate & 0.811 \\
  \hline
  True negative rate & 0.791 \\
  \hline
  False positive rate & 0.209 \\
  \hline
  False negative rate & 0.189 \\
  \hline
\end{tabular}
\caption{Final results of the facenet training after 12 hours on NVIDIA GeForce GTX 1080.}
\label{tab:facenet}
\end{table}

The facenet performance with the accuracy 81\% was not as good as we expected, but we need to keep in mind, that the task is very hard. The pairs of cars are shown in different lighting conditions and from different angles. It is not easy to say, if a vehicle from a front view is the same, as a different from a back view. Also merging the dataset had a lot of decisions to make, which vehicles are considered to be same and which are not. Since this is not the only parameter to decide reidentification, this performance is enough, but it is definitely something to improve in future work.

\subsubsection{Comparison to state of the art}
The state of the art methods compare vehicles by their license plates and therefore there is almost no research into vehicle similarity. There is a vehicle model classification \cite{munroe2005multi, psyllos2011vehicle, lai2001vehicle}, but they all require a good quality frontal view, which is not possible with high mounted fisheye cameras.

The original Facenet achieved a much higher performance on a face domain, which is a very loose comparison. Faces have more features than vehicles seen directly from above, and therefore are easier to distinguish.  Their dataset was also rotated and centered, while our approach was trained to deal with this variance. All features of the face were therefore seen, but vehicles were seen from different views and therefore some features remained hidden. The last difference is, that even people are often not able to distinguish vehicles, especially with very low resolution and similar models. 


\subsection{Multi camera tracking experiment}
\label{sec:multi-camera-tracking}

A city with the cameras built in lamps does not exist yet, since the project is still in development. Therefore the representation for the whole city was not tested. However, as was described in the section \ref{sec:reidentification}, from representing one edge, it is a very small step to representing the whole city. 

Only two videos of the same street at the same time from two different places was provided by the Brazilian party, that the concept was tested on. The reidentification part was described in the section \ref{sec:reidentification}. The detection and single camera tracking was described in the section \ref{sec:ssd-implementation} The score function described in the equation \ref{eq:score-reid} was thresholded with 0.01.  It should be stated, that because of the low fps, tracker sometimes created two tracks over similar trajectory. In such a case, only one track was used.

\subsubsection{Evaluation measurement}
The videos, that were provided, showed estimated 70 vehicles driving on the street during that time. The reidentification was tested each one of them. If the track was classified as the target vehicle correctly, that was measured as a true positive. It was not recognized, that was a false negative. If another track was falsely detected as the target one, that was considered to be a false positive. The total accuracy was computed as the ratio of true positives to all tests. 

\subsubsection{Results}
Out of 67 vehicles, that drove on the street, 59 were reidentified correctly. That corresponds to 88\% accuracy. Out of the 8 badly reidentified vehicles, 6 were false negatives, where the detector did not detect the cars and 2 were false positives, where a different car was marked as the target one.

\subsubsection{Comparison to state of the art}

As mentioned before, most of the state of the art multi-camera vehicle tracking approaches are very domain specific. Since the problem solved in this thesis contains fisheye cameras, no other method could be directly implemented and compared. They are either only for highways \cite{coifman2007vehicle, kuhne1991freeway}, rely on license plate recognition \cite{arth2007real, du2013automatic} or have very narrow field of view \cite{matei2011vehicle}.

\clearpage
\section{Discussion}
Artificial intelligence is said to be 20\% developing models and 80\% data management. Author of this thesis can only agree. Development of this thesis was very reliant on providing the data by the Brazilian party. However, only small amount of data were provided and very gradually. That is because the smart city project is still in development and only small amount of demo cameras was actually mounted. Combining lack of data with not reliable streaming, the process of development was sometimes frustrating. 

The reidentification experiments could be performed only on the data available. The only videos taken at the same time are the two views of the same street. Furthermore, the videos had different lengths and missing frames, which made the process of synchronization tedious. 

Because of the lack of data, experiments for the whole city, or even modeling intersections could not be done. However, the author of this thesis is confident, that the introduced models are accurate and robust enough to be able to reliably expand to a complicated graph of a real city.

Methods, such as image unwrapping into several less distorted images and detecting them separately, were explored and implemented but turned out to be a bad trade-off between performance and accuracy. This and several other approaches were not described in this work since they are too far from the assignment of the thesis.

\clearpage
\section{Conclusion}
This thesis combines solutions of several problems, such as object detection, single camera tracking, similarity metrics of vehicles, city representation and multi-camera tracking. Each of these tasks is very important and if solved poorly, could be a bottleneck of the whole process.

During development, the emphasis was put on computational speed and easy scalability of the project.

A thorough state of the art analysis of multiple fields of computer vision important for multi-camera tracking was described in the section \ref{sec:related_work}. This allowed educated selection of used methods.

The wide angle lens model and it's parameters were found using calibration data and transformations between real-world coordinates, camera coordinates and pixel coordinates in images were found.

An object detection and optical flow tracker were proposed and implemented for faster dataset generation for similarity training. Another dataset was created using standard annotation tools for object detection training.

For object detection was proposed and implemented improved architecture of the neural network SSD \cite{liu2016ssd} utilizing the video information and not just processing each frame independently. With the original SSD have been trained on the presented dataset. Experiments on fisheye camera domain show, that the proposed solution achieved 90.0\% mAP compared to 63.2\% state of the art SSD\cite{liu2016ssd} after one-day training. After 4 days training the network achieved 91.6\% mAP.

Google Facenet \cite{schroff2015facenet} was retrained from finding similarities between faces to vehicles achieving 80\% classification accuracy. It was trained and tested on a custom dataset.

A single-camera tracker based on optical flow provided by the Good Vision company was used for clustering detections over frames.

Vehicles in the city were represented by a probabilistic state space based on Markov chain. This allows easy expansion of the city with new cameras being installed as well as dealing with probability from not observed places and uncertainty. Reidentification of vehicles was proposed and implemented based on the computed similarity and relationships between cameras.

A real-world experiment was performed achieving the reidentification accuracy of 88\%.

\clearpage
\subsection{Future work}
This thesis is part of a long-term project with the Good Vision company. Author of this thesis will continue to develop and improve this project to the final state. With this being said, there are still areas to be improved and new approaches and models to be tried. Thanks to high modularization of this project, improved detectors, trackers and similarity metrics can be easily evaluated. Areas for further development include:

\begin{itemize}
\item Increase the size of both datasets for detection and similarity.
\item Deploy the system on a real city and make it accessible for the reinforcement agencies.
\item Instead of triplet loss for similarity training, implement and evaluate the recently introduced quadruplet loss \cite{chen2017beyond}.
\item Train and evaluate newer neural networks for object detection, such as \cite{lin2017focal, redmon2018yolov3}.
\item With increasing speed of segmentation networks create datasets and train segmentation models, which will improve tracking accuracy.
\end{itemize}

With newer and better neural networks introduced every year, this project will be gradually improved to keep up with state of the art methods in years to come.


\cleardoublepage
% Seznam literatury se nachází v bp.bib
\addcontentsline{toc}{section}{References}
\bibliographystyle{plain}
\bibliography{src/ref}{}
\cleardoublepage
\clearpage


% Apendixy, schemata, seznam zkratek, atp...
%\appendices
%\lhead{\emph{APPENDIX \leftmark}} 
%\input{src/obsah_CD}.
%\clearpage

%\clearpage
%\bibliographystyle{plain}
%\bibliography{src/ref}{}
%\cleardoublepage
%\clearpage

\end{document}